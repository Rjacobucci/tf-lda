{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation using TensorFlow (Batch VB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Latent Dirichlet Allocation model:\n",
    "\n",
    " - $\\theta_{d=1,...,M} \\sim \\mathrm{Dir}_K(\\alpha)$\n",
    " - $\\beta_{k=1,...,K} \\sim \\mathrm{Dir}_V(\\eta)$\n",
    " - $z_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ K}(\\theta_d)$\n",
    " - $w_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ V}(\\beta_{z_{di}})$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    " - $V$ is a vocabulary size\n",
    " - $K$ is a number of topics\n",
    " - $\\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution (known and symetric)\n",
    " - $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions (known and symetric)\n",
    " - $\\theta_d$ is the topic distribution for document d\n",
    " - $\\beta_k$ is the word distribution for topic k\n",
    " - $z_{di}$ is the topic for the i-th word in document d\n",
    " - $w_{di}$ is a specific word from d-th document belonging to $V$\n",
    "\n",
    "In plate notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hustlin_erd](LDA_plate_notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume full factorial design:\n",
    "\n",
    "\n",
    "$$q(z, \\beta, \\theta) = \\prod_d \\prod_i q(z_{di}) \\times \\prod_d q(\\theta_d) \\times \\prod_k q(\\beta_k)$$\n",
    "\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $q(z_{di}) = p(z_{di}|\\phi) = \\mathrm{Multinomial}_{ \\ K}(\\phi_{dw_{di}}) $\n",
    "- $q(\\theta_d) = p(\\theta_d|\\gamma) = \\mathrm{Dir}_{K}(\\gamma_{d}) $\n",
    "- $q(\\beta_k) = p(\\beta_k|\\lambda) = \\mathrm{Dir}_{V}(\\lambda_{k}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vocabulary to topic membership:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi = \\phi_{d=1,...,D, \\ v=1,...,V, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times V \\times K}$$\n",
    "$ \\forall_{d \\in 1,...,D} \\ \\phi_d $ is a *stochastic matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distribution of topics:*\n",
    "$$ \\gamma = \\gamma_{d=1,...,D, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Within topic vocabulary profile:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\lambda = \\lambda_{k=1,...,K \\ v=1,...,V} \\in \\mathbb{R}_{+}^{K \\times V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of the **E**vidence **L**ower **Bo**und:\n",
    "\n",
    "$$ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) := \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log p(w, z, \\theta, \\beta| \\alpha, \\eta) \\right] - \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log q(z, \\theta, \\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probability factorization of the LDA model\n",
    "\n",
    "$$ \\log p(w, z, \\theta, \\beta| \\alpha, \\eta) = \\log p(w|z, \\beta) p(z|\\theta) p(\\theta | \\alpha) p(\\beta | \\eta) = $$\n",
    "\n",
    "$$ = \\log \\left( \\prod_{k=1}^K p(\\beta_k|\\eta) \\times \\prod_{d=1}^D p(\\theta_d|\\alpha) \\times \\prod_{d=1}^{D} \\prod_{i=1}^{N_d} p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\log p(\\theta_d|\\alpha) + \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log \\left( p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) + \\frac{1}{D} \\sum_{k=1}^K \\log p(\\beta_k|\\eta) \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the expectations with respect to the posterior distribution results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log p(\\theta_d|\\alpha) \\right] = C_1(\\alpha) + \\sum_{k=1}^K (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\beta} \\left[ \\log p(w_{di}|\\beta_{z_{di}}) \\right] =  \\mathbb{E}_{z, \\beta} \\left[ \\log \\beta_{z_{di} i} \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\theta} \\left[ \\log p(z_{di}|\\theta_d) \\right] =  \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log p(\\beta_{k}|\\eta) \\right] = C_2(\\eta) + \\sum_{v=1}^{V} (\\eta_v - 1) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entropy of the posterior distribution we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log\\left(\\prod_{d=1}^{D} \\prod_{i=1}^{N_{d}} q(z_{di}) \\times \\prod_{d=1}^D q(\\theta_d) \\times \\prod_{k=1}^{K} q(\\beta_k)\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\left( \\sum_{i=1}^{N_{d}} \\log q(z_{di}) + \\log q(\\theta_d) + \\frac{1}{D} \\sum_{k=1}^{K} \\log q(\\beta_k)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and expectations with respect to the posterior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z} \\left[ \\log q(z_{di})  \\right] = \\mathbb{E}_{z_{di}} \\left[ \\mathbb{1}_{[z_{di}=k]} \\log \\phi_{dw_{di}k}  \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\log \\phi_{dw_{di}k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log q(\\theta_d)  \\right] = \\mathbb{E}_{\\theta} \\left[ \\log \\frac{1}{B(\\gamma_d)} \\prod_{k=1}^{K} \\theta_{dk}^{\\gamma_{dk} - 1} \\right] = -\\log B(\\gamma_d) + \\sum_{k=1}^K (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta}\\left[ \\log q(\\beta_k) \\right] = -\\log B(\\lambda_k) + \\sum_{v=1}^V (\\lambda_{kv} - 1)\\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We should then be able to collapse ELBO to the formula based on counts of words rather than words from documents. This will show that the LDA model is fixed size method.* \n",
    "\n",
    "*\"When training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\"* (Data Camp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping terms by corresponding parameters we get:\n",
    "\n",
    "#### Document vocabulary membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\phi_{dv}]} = \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\sum_{k=1}^K \\phi_{dw_{di}k} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dw_{di}k}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n_{dv}$ is a number of times word $v$ from vocabulary $V$ was counted in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\gamma_{d}]} = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\log B(\\gamma_d) - (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{D} \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K \\left\\{ (\\eta_v - \\lambda_{kv}) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k} \\right] + \\frac{1}{V} \\log B(\\lambda_k) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate ascent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for every document and vocabulary entry separately we get:\n",
    "\n",
    "#### Document vocabulary membership:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\mathcal{L}_{[\\phi_{dv}]} =  \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ =\\mathrm{argmax}_{\\phi_{dv} \\in \\mathbb{R}_{+}^K} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) + \\xi (\\sum_{k=1}^K \\phi_{dvk} - 1) = \\Psi(\\phi_{dv}, \\xi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\xi} \\Psi(\\phi_{dv}, \\xi) = \\sum_{k=1}^K \\phi_{dvk} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = 0 \\iff n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 + \\xi' = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\log \\phi_{dvk} = \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - 1 + \\xi'  \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doucuments' distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$ \\log B(\\gamma_d) = \\log \\frac{\\prod_{k=1}^{K}\\Gamma(\\gamma_{dk})}{\\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})} = \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\mathcal{L}_{[\\gamma_{d}]} = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} + \\log B(\\gamma_d) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) (\\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}}) = \\Psi(\\gamma_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = -\\psi(\\gamma_{dj}) + \\psi({\\sum_k \\gamma_{dk})} + \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\psi(\\gamma_{dj}) - \\psi(\\sum_k \\gamma_{dk}) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *trigamma* function has no real roots it follows that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = 0 \\iff \\forall_{k} \\ \\ \\  \\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk} = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff   \\gamma_{dk} = \\alpha_k  + \\sum_{v=1}^V n_{dv} \\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile (M-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact derivation as for the $\\gamma_d$ gives an update equation for $\\lambda_k$\n",
    "\n",
    "$$ \\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - initialize $\\lambda$ and $\\gamma$\n",
    " - while improvement $ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) > 1e-6$ \n",
    "     - while $\\gamma$ changes:\n",
    "         - for $d = 1,...,D$\n",
    "         - $\\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\}$\n",
    "         - $\\gamma_{dk} = \\alpha  + \\sum_{v=1}^V n_{dv} \\phi_{dvk}$\n",
    "     - $\\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Static Graph Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "v1.6.0-0-gd2e24b6039\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.__git_version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=10.0)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "res = x + y\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "# feed data and run TF graph\n",
    "with tf.Session() as sees:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sees.run(res, feed_dict={y: 3.0}))\n",
    "    print(sees.run(res, feed_dict={y: 2.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Eager Execution Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "v1.6.0-0-gd2e24b6039\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.__git_version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(13.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tfe.Variable(initial_value=10.0)\n",
    "y = 3.0\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + y).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Synthetic DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data dimensions\n",
    "K = 3\n",
    "V = 10\n",
    "D = 1000\n",
    "N = 100\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(2014)\n",
    "\n",
    "# beta prior parameters\n",
    "eta = np.ones(V) * 1e-1\n",
    "\n",
    "# beta profiles\n",
    "beta = np.random.dirichlet(alpha=eta, size=K)\n",
    "\n",
    "# theta prior parameters\n",
    "alpha = np.ones(K) * 1e-1\n",
    "# alpha[0] = 10\n",
    "\n",
    "# document's prior topic allocation\n",
    "theta = np.random.dirichlet(alpha=alpha, size=D)\n",
    "\n",
    "# word's topic membership\n",
    "z = [np.random.choice(K, size=N, replace=True, p=theta[d, :]) for d in range(D)]\n",
    "z = np.vstack(z)\n",
    "\n",
    "# actual words and counts\n",
    "w = [np.array([np.random.choice(V, size=1, p=beta[k,:])[0] for k in z[d, :]]  + list(range(V))) for d in range(D)]\n",
    "nw = [np.unique(w[d], return_counts=True)[1] for d in range(D)]\n",
    "nw = np.vstack(nw) - 1.0\n",
    "w = np.vstack(w)\n",
    "\n",
    "nw = tf.convert_to_tensor(nw, dtype=tf.float32)\n",
    "nw = tfe.Variable(initial_value=tf.transpose(nw),\n",
    "                 name=\"nw_vd\")\n",
    "\n",
    "nw_kvd = tf.tile(tf.expand_dims(nw / tf.reduce_sum(nw), axis=0), \n",
    "                 multiples=[K, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.927</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2\n",
       "0  0.000  0.579  0.041\n",
       "1  0.000  0.010  0.000\n",
       "2  0.000  0.000  0.000\n",
       "3  0.063  0.003  0.001\n",
       "4  0.000  0.033  0.460\n",
       "5  0.927  0.006  0.079\n",
       "6  0.001  0.363  0.000\n",
       "7  0.008  0.000  0.086\n",
       "8  0.000  0.000  0.333\n",
       "9  0.000  0.006  0.000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"beta:\")\n",
    "pd.DataFrame(np.round(np.transpose(beta), decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.215</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.739</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.981</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2\n",
       "0  0.215  0.002  0.782\n",
       "1  0.000  0.000  1.000\n",
       "2  0.739  0.261  0.000\n",
       "3  0.047  0.535  0.418\n",
       "4  0.240  0.000  0.760\n",
       "5  0.981  0.019  0.000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"theta:\")\n",
    "pd.DataFrame(np.round(theta, decimals=3)).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents word counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3     4     5     6     7     8    9\n",
       "0   0.0  0.0  0.0  2.0  33.0  31.0   0.0   5.0  29.0  0.0\n",
       "1   7.0  0.0  0.0  0.0  40.0   9.0   0.0  12.0  32.0  0.0\n",
       "2  14.0  0.0  0.0  6.0   1.0  68.0   9.0   2.0   0.0  0.0\n",
       "3  29.0  1.0  0.0  0.0  25.0   7.0  22.0   4.0  12.0  0.0\n",
       "4   4.0  0.0  0.0  0.0  42.0  17.0   0.0  10.0  27.0  0.0\n",
       "5   0.0  0.0  0.0  6.0   0.0  94.0   0.0   0.0   0.0  0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"documents word counts:\")\n",
    "pd.DataFrame(tf.transpose(nw).numpy()).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model\n",
    "\n",
    " - initialize parameters\n",
    " - updating functions:\n",
    "     - update gamma\n",
    "     - upate phi (with e_log_beta, e_log_theta)\n",
    "     - update lambda\n",
    " - main unction (alternating E and M step)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LDA parameters\n",
    "def initialize_variables(K, V, D, alpha=1e-3, eta=1e-3, seed=2014):\n",
    "    \"\"\"\n",
    "    Initialize parameters of LDA model returning adequate Tensors.\n",
    "\n",
    "    args:\n",
    "    \n",
    "        K (int): number of LDA components \n",
    "        V (int): vocabulary size\n",
    "        D (int): number of documents\n",
    "        alpha (float): hyperparameter for theta prior\n",
    "        eta (float): hyperparameter for beta prior\n",
    "       \n",
    "       \n",
    "    returns:\n",
    "    \n",
    "        eta: [V] tensor with prior parameters (alpha) for beta\n",
    "        lambda: [K, V] tensor with posterior word distribution per class\n",
    "        phi: [K, V, D] tensor with vocabulary membership per document\n",
    "        gamma: [K, D] tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(seed)\n",
    "    eta = tfe.Variable(initial_value=tf.ones(V) * eta, \n",
    "                       name=\"eta_v\")\n",
    "    alpha = tfe.Variable(initial_value=tf.ones(K) * alpha, \n",
    "                         name=\"alpha_k\")    \n",
    "    lam = tfe.Variable(initial_value=tf.abs(tf.random_normal(shape=(K, V))), \n",
    "                       name=\"lambda_kv\")\n",
    "    \n",
    "    phi = tfe.Variable(initial_value=tf.random_normal(shape=(K, V, D)), \n",
    "                       name=\"phi_kvd\")\n",
    "    tf.assign(ref=phi, value=tf.nn.softmax(phi, axis=0))\n",
    "    \n",
    "    gamma = tfe.Variable(initial_value=tf.abs(tf.random_normal(shape=(K, D))), \n",
    "                        name=\"gamma_kd\")\n",
    "    \n",
    "    e_log_beta = tfe.Variable(initial_value=tf.abs(tf.random_normal(shape=(K, V, D))) * .0, \n",
    "                        name=\"e_log_beta_kvd\")\n",
    "    \n",
    "    e_log_theta = tfe.Variable(initial_value=tf.abs(tf.random_normal(shape=(K, V, D))) * .0, \n",
    "                        name=\"e_log_theta_kvd\")\n",
    "    \n",
    "    return eta, alpha, lam, phi, gamma, e_log_beta, e_log_theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lambda update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lambda(lam, eta, phi, nw):\n",
    "    \n",
    "    K = lam.shape.as_list()[0]\n",
    "    for k in range(K):\n",
    "        tf.scatter_update(ref=lam, \n",
    "                  indices=k, \n",
    "                  updates=tf.reduce_sum(tf.multiply(phi[k], nw), axis=1) + eta)\n",
    "        \n",
    "    return lam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gamma update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\gamma_{dk} = \\alpha  + \\sum_{v=1}^V n_{dv} \\phi_{dvk}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma update\n",
    "def update_gamma(gamma, alpha, phi, nw):\n",
    "    \n",
    "    K = gamma.shape.as_list()[0]\n",
    "    for k in range(K):\n",
    "        tf.scatter_update(ref=gamma, \n",
    "                  indices=k, \n",
    "                  updates=tf.reduce_sum(tf.multiply(phi[k], nw), axis=0) + alpha[k])\n",
    "\n",
    "        \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e_log_beta update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right]  = \\psi(\\lambda_{kv}) - \\psi(\\sum_{v=1}^V \\lambda_{kv}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_e_log_beta(e_log_beta, lam):\n",
    "    \n",
    "    K = lam.shape.as_list()[0]\n",
    "    for k in range(K):\n",
    "        tf.scatter_update(ref=e_log_beta,\n",
    "                  indices=k,\n",
    "                  updates=tf.tile(tf.expand_dims(tf.digamma(lam[k]) - tf.digamma(tf.reduce_sum(lam[k])), axis=1), multiples=[1, D]))\n",
    "    \n",
    "    return e_log_beta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e_log_theta update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_e_log_theta(e_log_theta, gamma):\n",
    "    \n",
    "    tf.assign(ref=e_log_theta, \n",
    "              value=tf.tile(tf.expand_dims(tf.digamma(gamma) - tf.digamma(tf.reduce_sum(gamma, axis=0)), axis=1), multiples=[1, V, 1]))\n",
    "\n",
    "    return e_log_theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### phi update:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi(e_log_beta, e_log_theta):\n",
    "    tf.assign(ref=phi, \n",
    "              value=e_log_beta + e_log_theta)\n",
    "    tf.assign(ref=phi, value=tf.nn.softmax(logits=phi, axis=0))\n",
    "    \n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A := \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ B := \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo(phi, e_log_beta, e_log_theta, nw_kvd):\n",
    "\n",
    "    A = tf.reduce_sum(nw_kvd * phi * (e_log_beta + e_log_theta - tf.log(phi + 1e-6)))    \n",
    "    return A.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it all up together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ELBO: -3.6008358\n",
      "Iteration: 1 ELBO: -1.4867418\n",
      "Iteration: 2 ELBO: -1.4736624\n",
      "Iteration: 3 ELBO: -1.452014\n",
      "Iteration: 4 ELBO: -1.3735723\n",
      "Iteration: 5 ELBO: -1.2862656\n",
      "Iteration: 6 ELBO: -1.2823579\n",
      "Iteration: 7 ELBO: -1.2821975\n",
      "Iteration: 8 ELBO: -1.2821814\n",
      "Iteration: 9 ELBO: -1.2821766\n",
      "Iteration: 10 ELBO: -1.2821758\n",
      "Converged!\n"
     ]
    }
   ],
   "source": [
    "seed += 1\n",
    "eta, alpha, lam, phi, gamma, e_log_beta, e_log_theta = initialize_variables(K, V, D)\n",
    "\n",
    "iter_elbo = []\n",
    "prev_elbo = 0.0\n",
    "next_elbo = 0.0\n",
    "iter = 0\n",
    "\n",
    "for i in range(100000):\n",
    "    \n",
    "    for j in range(100000):\n",
    "        # E-Step:\n",
    "        update_e_log_beta(e_log_beta, lam);\n",
    "        update_e_log_theta(e_log_theta, gamma);\n",
    "        update_phi(e_log_theta=e_log_theta, e_log_beta=e_log_beta)\n",
    "        gamma_prev = gamma.value()\n",
    "        update_gamma(gamma, alpha, phi, nw)\n",
    "        \n",
    "        diff = tf.reduce_mean(tf.abs(gamma_prev - gamma.value()))\n",
    "        if diff < 1e-6:\n",
    "            break\n",
    "    \n",
    "    # M-Step:\n",
    "    update_lambda(lam, eta, phi, nw)\n",
    "    \n",
    "    \n",
    "    next_elbo = elbo(phi, e_log_beta, e_log_theta, nw_kvd)\n",
    "    iter_elbo.append(next_elbo)\n",
    "\n",
    "    print(\"Iteration:\", iter, \"ELBO:\", next_elbo)\n",
    "    \n",
    "    diff = np.abs(next_elbo - prev_elbo)\n",
    "    if diff < 1e-6:\n",
    "        print(\"Converged!\")\n",
    "        break\n",
    "    else:\n",
    "        iter += 1\n",
    "        prev_elbo = next_elbo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1223cc390>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "pd.DataFrame(iter_elbo, columns=['elbo']).reset_index().plot.line(x=\"index\", y=\"elbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.927</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2\n",
       "0  0.000  0.579  0.041\n",
       "1  0.000  0.010  0.000\n",
       "2  0.000  0.000  0.000\n",
       "3  0.063  0.003  0.001\n",
       "4  0.000  0.033  0.460\n",
       "5  0.927  0.006  0.079\n",
       "6  0.001  0.363  0.000\n",
       "7  0.008  0.000  0.086\n",
       "8  0.000  0.000  0.333\n",
       "9  0.000  0.006  0.000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"beta:\")\n",
    "topic_term_dist = (tf.transpose(tf.transpose(lam) / tf.reduce_sum(lam, axis=1))).numpy()\n",
    "# pd.DataFrame(np.round(np.transpose(beta), decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate results for visualisation of topics\n",
    "\n",
    "https://pyldavis.readthedocs.io/en/latest/modules/API.html#pyLDAvis.prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic term distribution:\n",
    "topic_term_dist = np.round(np.vstack([(lam / tf.reduce_sum(lam)).numpy()  for lam in lambda_]), decimals=3)\n",
    "topic_term_dist\n",
    "\n",
    "# doc_topic_dists :array-like, shape (n_docs, n_topics)\n",
    "doc_topic_dist = tf.stack([tf.reshape(g_k, shape=(1000, )) for g_k in gamma], axis=1)\n",
    "doc_topic_dist = doc_topic_dist / tf.reduce_sum(doc_topic_dist, axis=1, keep_dims=True)\n",
    "doc_topic_dist = doc_topic_dist.numpy()\n",
    "\n",
    "# doc_lengths :array-like, shape n_docs\n",
    "doc_len = tf.reduce_sum(nw, axis=1)\n",
    "doc_len = doc_len.numpy()\n",
    "\n",
    "# vocab :array-like, shape n_terms\n",
    "vocab = np.array(list(range(V)))\n",
    "\n",
    "# term_frequency :array-like, shape n_terms\n",
    "term_frec = tf.reduce_sum(nw, axis=0)\n",
    "term_frec = term_frec.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qba/.local/share/virtualenvs/LDA-pHQnYWLy/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "topics = pyLDAvis.prepare(topic_term_dist, doc_topic_dist, doc_len, vocab, term_frec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(topics, fileobj=\"results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "The dataset is in the form of a 11463 x 5812 matrix of word counts, containing 11463 words and 5811 NIPS conference papers (the first column contains the list of words). Each column contains the number of times each word appears in the corresponding document. The names of the columns give information about each document and its timestamp in the following format: Xyear_paperID. \n",
    "\n",
    "The matrix of word counts was obtained using the R package 'tmâ€ to process the raw .txt files of the full text of the NIPS conference papers published between 1987 and 2015. The document-term matrix was constructed after tokenization, removal of stopwords and truncation of the vocabulary by only keeping words occurring more than 50 times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# download dataset\n",
    "! curl https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv > NIPS_1987-2015.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"NIPS_1987-2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = np.array(data.iloc[:, 1:])\n",
    "nw = nw.astype('float32')\n",
    "nw = nw.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = tf.convert_to_tensor(nw)\n",
    "nw = nw[0:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "D, V = nw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "eta, alpha, lambda_, phi, gamma = initialize_parameters(K, V, D, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ELBO: -12.418982\n",
      "12.02796196937561\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "def lda(eta=eta, alpha=alpha, lambda_=lambda_, phi=phi, gamma=gamma, K=K, V=V, D=D, seed=seed):\n",
    "\n",
    "    prev_elbo = 0.0\n",
    "    next_elbo = 0.0\n",
    "    iter = 0\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "        for j in range(1):\n",
    "            # E-Step:\n",
    "            phi = update_phi(lambda_, gamma, D, V)\n",
    "            gamma_next = update_gamma(gamma, alpha, phi, nw)\n",
    "\n",
    "            diff = 0.0\n",
    "            diff = np.mean([tf.reduce_sum(tf.abs(g_prev - g_next)).numpy() for g_prev, g_next in zip(gamma, gamma_next)])\n",
    "            gamma = gamma_next\n",
    "            if diff < 1e-6:\n",
    "                break\n",
    "\n",
    "        # M-Step:\n",
    "        lambda_ = update_lambda(lambda_, eta, phi, nw)\n",
    "\n",
    "\n",
    "        next_elbo = elbo(lambda_, gamma, D, V, nw)\n",
    "        print(\"Iteration:\", iter, \"ELBO:\", next_elbo)\n",
    "\n",
    "        diff = np.abs(next_elbo - prev_elbo)\n",
    "        if diff < 1e-6:\n",
    "            print(\"Converged!\")\n",
    "            break\n",
    "        else:\n",
    "            iter += 1\n",
    "            prev_elbo = next_elbo\n",
    "\n",
    "        \n",
    "lda()            \n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ELBO: -12.421164\n",
      "         16394 function calls (16392 primitive calls) in 12.195 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      319   11.713    0.037   11.713    0.037 {built-in method _pywrap_tensorflow_internal.TFE_Py_Execute}\n",
      "        1    0.190    0.190    7.068    7.068 <ipython-input-12-23890523edae>:1(elbo)\n",
      "        1    0.122    0.122   12.176   12.176 <ipython-input-26-0e2d6dc2d4aa>:3(lda)\n",
      "        1    0.056    0.056    0.582    0.582 <ipython-input-8-371989c98d7e>:2(update_gamma)\n",
      "        1    0.056    0.056    0.602    0.602 <ipython-input-7-62df51f3fab9>:2(update_lambda)\n",
      "        1    0.020    0.020    3.800    3.800 <ipython-input-11-b39e0c75cc23>:1(update_phi)\n",
      "        1    0.019    0.019   12.195   12.195 <string>:1(<module>)\n",
      "      179    0.001    0.000    0.002    0.000 constant_op.py:89(convert_to_eager_tensor)\n",
      "     3592    0.001    0.000    0.001    0.000 {built-in method builtins.isinstance}\n",
      "      208    0.001    0.000    0.003    0.000 ops.py:953(internal_convert_to_tensor)\n",
      "  435/433    0.001    0.000    0.002    0.000 execute.py:166(args_to_matching_eager)\n",
      "      319    0.001    0.000   11.714    0.037 execute.py:33(quick_execute)\n",
      "       30    0.001    0.000    0.758    0.025 array_ops.py:458(_slice_helper)\n",
      "       58    0.001    0.000    0.924    0.016 math_ops.py:921(binary_op_wrapper)\n",
      "       72    0.001    0.000    0.100    0.001 gen_math_ops.py:4836(_sum)\n",
      "      184    0.000    0.000    0.002    0.000 tensor_shape.py:423(__init__)\n",
      "      319    0.000    0.000    0.001    0.000 backprop.py:241(_record_gradient)\n",
      "       72    0.000    0.000    0.104    0.001 math_ops.py:1315(reduce_sum)\n",
      "       30    0.000    0.000    0.754    0.025 gen_array_ops.py:5236(strided_slice)\n",
      "      270    0.000    0.000    0.000    0.000 tensor_shape.py:29(__init__)\n",
      "      184    0.000    0.000    0.002    0.000 ops.py:783(shape)\n",
      "      107    0.000    0.000    0.000    0.000 ops.py:5580(__enter__)\n",
      "      619    0.000    0.000    0.000    0.000 context.py:241(in_graph_mode)\n",
      "       80    0.000    0.000    0.009    0.000 gen_math_ops.py:1404(digamma)\n",
      "      107    0.000    0.000    0.000    0.000 ops.py:5566(__init__)\n",
      "       22    0.000    0.000    0.512    0.023 gen_math_ops.py:2772(_mul)\n",
      "       72    0.000    0.000    0.002    0.000 math_ops.py:1284(_ReductionDims)\n",
      "      319    0.000    0.000    0.001    0.000 tape.py:90(could_possibly_record)\n",
      "      179    0.000    0.000    0.002    0.000 constant_op.py:134(constant)\n",
      "      319    0.000    0.000    0.000    0.000 {built-in method _pywrap_tensorflow_internal.TFE_Py_TapeSetIsEmpty}\n",
      "      270    0.000    0.000    0.001    0.000 tensor_shape.py:383(as_dimension)\n",
      "      244    0.000    0.000    0.000    0.000 ops.py:663(dtype)\n",
      "      905    0.000    0.000    0.000    0.000 context.py:469(context)\n",
      "       74    0.000    0.000    2.103    0.028 deprecation.py:398(new_func)\n",
      "       24    0.000    0.000    0.488    0.020 gen_math_ops.py:165(add)\n",
      "      121    0.000    0.000    0.000    0.000 context.py:499(in_graph_mode)\n",
      "       53    0.000    0.000    0.152    0.003 gen_math_ops.py:4802(_sub)\n",
      "       30    0.000    0.000    0.755    0.025 array_ops.py:645(strided_slice)\n",
      "        2    0.000    0.000    0.011    0.006 <ipython-input-9-fffc08e77f94>:2(<listcomp>)\n",
      "      101    0.000    0.000    0.000    0.000 array_ops.py:988(_get_dtype_from_nested_lists)\n",
      "       72    0.000    0.000    0.001    0.000 math_ops.py:1307(_may_reduce_to_scalar)\n",
      "      107    0.000    0.000    0.000    0.000 ops.py:5626(__exit__)\n",
      "      152    0.000    0.000    0.000    0.000 dtypes.py:269(__eq__)\n",
      "      315    0.000    0.000    0.000    0.000 context.py:245(in_eager_mode)\n",
      "      184    0.000    0.000    0.001    0.000 tensor_shape.py:458(<listcomp>)\n",
      "      498    0.000    0.000    0.000    0.000 context.py:273(device_name)\n",
      "      168    0.000    0.000    0.002    0.000 ops.py:891(convert_to_tensor)\n",
      "      509    0.000    0.000    0.000    0.000 context.py:199(_handle)\n",
      "       52    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
      "      645    0.000    0.000    0.000    0.000 {method '_datatype_enum' of 'EagerTensor' objects}\n",
      "       72    0.000    0.000    0.000    0.000 tensor_shape.py:788(is_fully_defined)\n",
      "        2    0.000    0.000    0.003    0.001 <ipython-input-10-ab8b2b872258>:2(<listcomp>)\n",
      "      156    0.000    0.000    0.000    0.000 execute.py:96(make_int)\n",
      "      125    0.000    0.000    0.001    0.000 constant_op.py:232(_constant_tensor_conversion_function)\n",
      "      214    0.000    0.000    0.000    0.000 context.py:258(scope_name)\n",
      "       94    0.000    0.000    0.003    0.000 array_ops.py:885(stack)\n",
      "      141    0.000    0.000    0.000    0.000 dtypes.py:102(base_dtype)\n",
      "       20    0.000    0.000    0.237    0.012 math_ops.py:326(multiply)\n",
      "      155    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "      101    0.000    0.000    0.001    0.000 array_ops.py:1009(_autopacking_conversion_function)\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        5    0.000    0.000    7.165    1.433 gen_array_ops.py:5565(tile)\n",
      "      198    0.000    0.000    0.000    0.000 dtypes.py:650(as_dtype)\n",
      "      293    0.000    0.000    0.000    0.000 dtypes.py:134(as_datatype_enum)\n",
      "        9    0.000    0.000    0.000    0.000 socket.py:333(send)\n",
      "      184    0.000    0.000    0.000    0.000 {method '_shape_tuple' of 'EagerTensor' objects}\n",
      "      235    0.000    0.000    0.000    0.000 ops.py:125(is_dense_tensor_like)\n",
      "      112    0.000    0.000    0.001    0.000 ops.py:787(get_shape)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:366(write)\n",
      "       72    0.000    0.000    0.000    0.000 execute.py:114(make_bool)\n",
      "        1    0.000    0.000    0.001    0.001 <ipython-input-26-0e2d6dc2d4aa>:17(<listcomp>)\n",
      "        1    0.000    0.000    0.532    0.532 gen_math_ops.py:2310(log)\n",
      "      184    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "      112    0.000    0.000    0.000    0.000 tensor_shape.py:790(<genexpr>)\n",
      "       10    0.000    0.000    0.000    0.000 math_ops.py:239(abs)\n",
      "        6    0.000    0.000    0.002    0.000 gen_array_ops.py:2794(_pack)\n",
      "      120    0.000    0.000    0.000    0.000 tensor_shape.py:476(ndims)\n",
      "      246    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "      141    0.000    0.000    0.000    0.000 dtypes.py:89(_is_ref_dtype)\n",
      "      107    0.000    0.000    0.000    0.000 context.py:253(scope_name)\n",
      "       11    0.000    0.000    0.000    0.000 ops.py:760(_copy)\n",
      "       11    0.000    0.000    0.000    0.000 {method '_copy_to_device' of 'EagerTensor' objects}\n",
      "       31    0.000    0.000    0.000    0.000 dtypes.py:279(__ne__)\n",
      "        4    0.000    0.000    0.000    0.000 gen_array_ops.py:3830(reshape)\n",
      "        2    0.000    0.000    1.998    0.999 nn_ops.py:1638(_softmax)\n",
      "      132    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        9    0.000    0.000    0.000    0.000 iostream.py:195(schedule)\n",
      "       11    0.000    0.000    0.000    0.000 {method '_numpy' of 'EagerTensor' objects}\n",
      "       11    0.000    0.000    0.000    0.000 ops.py:669(numpy)\n",
      "        4    0.000    0.000    0.000    0.000 array_ops.py:288(shape_internal)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:53(_mean)\n",
      "        4    0.000    0.000    0.000    0.000 gen_array_ops.py:4435(shape)\n",
      "       10    0.000    0.000    0.000    0.000 gen_math_ops.py:24(_abs)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:943(_autopacking_helper)\n",
      "        2    0.000    0.000    0.001    0.000 nn_ops.py:1611(_flatten_outer_dims)\n",
      "        2    0.000    0.000    1.997    0.999 gen_nn_ops.py:4511(_softmax)\n",
      "        2    0.000    0.000    1.998    0.999 nn_ops.py:1715(softmax)\n",
      "        1    0.000    0.000   12.195   12.195 {built-in method builtins.exec}\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "       74    0.000    0.000    0.000    0.000 deprecation.py:504(deprecated_argument_lookup)\n",
      "        1    0.000    0.000    0.009    0.009 math_ops.py:1014(_truediv_python3)\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:434(rank_internal)\n",
      "        2    0.000    0.000    0.000    0.000 gen_array_ops.py:602(_concat_v2)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "       10    0.000    0.000    0.000    0.000 dtypes.py:157(is_complex)\n",
      "        7    0.000    0.000    0.000    0.000 dtypes.py:246(is_compatible_with)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:300(_is_master_process)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2854(mean)\n",
      "       28    0.000    0.000    0.000    0.000 context.py:249(scalar_cache)\n",
      "       11    0.000    0.000    0.000    0.000 tape.py:79(record_operation)\n",
      "       40    0.000    0.000    0.000    0.000 tensor_shape.py:80(value)\n",
      "        1    0.000    0.000    0.000    0.000 gen_array_ops.py:1185(_expand_dims)\n",
      "       11    0.000    0.000    0.000    0.000 ops.py:800(cpu)\n",
      "        2    0.000    0.000    0.000    0.000 gen_array_ops.py:4563(_slice)\n",
      "        9    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
      "        4    0.000    0.000    0.000    0.000 execute.py:121(make_type)\n",
      "        1    0.000    0.000    0.009    0.009 gen_math_ops.py:3371(_real_div)\n",
      "        2    0.000    0.000    0.275    0.137 math_ops.py:1157(_mul_dispatch)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:43(_count_reduce_items)\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:592(slice)\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:1081(concat)\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:313(_schedule_flush)\n",
      "        2    0.000    0.000    0.003    0.001 <ipython-input-10-ab8b2b872258>:1(e_log_theta)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:401(rank)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _pywrap_tensorflow_internal.TFE_Py_TapeSetRecordOperation}\n",
      "        4    0.000    0.000    0.000    0.000 execute.py:195(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:495(asanyarray)\n",
      "        4    0.000    0.000    0.000    0.000 array_ops.py:262(shape)\n",
      "        1    0.000    0.000    0.000    0.000 array_ops.py:142(expand_dims)\n",
      "        2    0.000    0.000    0.000    0.000 math_ops.py:346(subtract)\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "        4    0.000    0.000    0.000    0.000 tensor_shape.py:68(__int__)\n",
      "        2    0.000    0.000    0.011    0.006 <ipython-input-9-fffc08e77f94>:1(e_log_beta)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:297(__hash__)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prof = cProfile.run('lda()', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic term distribution:\n",
    "topic_term_dist = np.vstack([(lam / tf.reduce_sum(lam)).numpy()  for lam in lambda_])\n",
    "topic_term_dist\n",
    "\n",
    "# doc_topic_dists :array-like, shape (n_docs, n_topics)\n",
    "doc_topic_dist = tf.stack([tf.reshape(g_k, shape=(g_k.shape[0], )) for g_k in gamma], axis=1)\n",
    "doc_topic_dist = doc_topic_dist / tf.reduce_sum(doc_topic_dist, axis=1, keep_dims=True)\n",
    "doc_topic_dist = doc_topic_dist.numpy()\n",
    "\n",
    "# doc_lengths :array-like, shape n_docs\n",
    "doc_len = tf.reduce_sum(nw, axis=1)\n",
    "doc_len = doc_len.numpy()\n",
    "\n",
    "# vocab :array-like, shape n_terms\n",
    "vocab = data.iloc[:,0]\n",
    "\n",
    "# term_frequency :array-like, shape n_terms\n",
    "term_frec = tf.reduce_sum(nw, axis=0)\n",
    "term_frec = term_frec.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qba/.local/share/virtualenvs/LDA-pHQnYWLy/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "topics = pyLDAvis.prepare(topic_term_dist, doc_topic_dist, doc_len, vocab, term_frec)\n",
    "pyLDAvis.save_html(topics, fileobj=\"results.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_term_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite using updates instead of stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>\n",
      "tf.Tensor([-1. -1. -1.  1.  1.  1.  1.  1.  1.  1.], shape=(10,), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=array([-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "ref = tf.fill(value=1.0, dims=(10, ))\n",
    "ref = tfe.Variable(ref)\n",
    "upd = tf.fill(value=-1.0, dims=(3, ))\n",
    "print(ref)\n",
    "print(tf.scatter_update(ref=ref, indices=tf.convert_to_tensor([0, 1, 2]), updates=upd))\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=102, shape=(3,), dtype=float32, numpy=array([-1., -1., -1.], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=117, shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LDA parameters\n",
    "def initialize_variables(K, V, D, alpha=1e-1, eta=1e-1, seed=2014):\n",
    "    \"\"\"\n",
    "    Initialize parameters of LDA model returning adequate Tensors.\n",
    "\n",
    "    args:\n",
    "    \n",
    "        K (int): number of LDA components \n",
    "        V (int): vocabulary size\n",
    "        D (int): number of documents\n",
    "        alpha (float): hyperparameter for theta prior\n",
    "        eta (float): hyperparameter for beta prior\n",
    "       \n",
    "       \n",
    "    returns:\n",
    "    \n",
    "        eta: [V] tensor with prior parameters (alpha) for beta\n",
    "        lambda: [V, K] tensor with posterior word distribution per class\n",
    "        phi: [D, V, K] tensor with vocabulary membership per document\n",
    "        gamma: [D, K] tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    eta = tfe.Variable(tf.ones(V) * eta, name=\"eta\")\n",
    "    alpha = tfe.Variable(tf.ones(K) * alpha, name=\"alpha\")\n",
    "    lambda_ = tfe.Variable(tf.abs(tf.random_normal(shape=(V, K))), name='lambda')\n",
    "    \n",
    "    phi = tfe.Variable(tf.random_normal(shape=(D, V, K)), name=\"phi\")\n",
    "    phi = tf.nn.softmax(phi, axis=2)\n",
    "    \n",
    "    gamma = tfe.Variable(tf.abs(tf.random_normal(shape=(D, K))))\n",
    "    \n",
    "    return eta, alpha, lambda_, phi, gamma\n",
    "\n",
    "# test\n",
    "eta, alpha, lambda_, phi, gamma = initialize_variables(K, V, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'lambda:0' shape=(10, 3) dtype=float32, numpy=\n",
       "array([[0.30049554, 1.3264463 , 0.3345    ],\n",
       "       [0.41716772, 0.47891214, 0.60391957],\n",
       "       [0.52135056, 1.1167537 , 2.0634964 ],\n",
       "       [0.3132391 , 0.06342854, 0.98612607],\n",
       "       [0.07255968, 1.0613515 , 0.6355119 ],\n",
       "       [0.07127264, 1.4961201 , 0.63234806],\n",
       "       [0.8790525 , 0.26309258, 0.26927882],\n",
       "       [0.4351647 , 2.0731583 , 0.9537371 ],\n",
       "       [1.8273319 , 0.38898602, 0.48102027],\n",
       "       [0.6974121 , 2.812919  , 0.0922653 ]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda update\n",
    "def update_lambda(lambda_, eta, phi, nw):\n",
    "    \n",
    "    K = len(lambda_)\n",
    "    for k in range(K):\n",
    "        lambda_[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=0, keepdims=True), eta)\n",
    "        \n",
    "    return lambda_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,0], nw * 0.0), axis=0, keepdims=False), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=224, shape=(10, 2), dtype=int32, numpy=\n",
       "array([[0, 0],\n",
       "       [1, 0],\n",
       "       [2, 0],\n",
       "       [3, 0],\n",
       "       [4, 0],\n",
       "       [5, 0],\n",
       "       [6, 0],\n",
       "       [7, 0],\n",
       "       [8, 0],\n",
       "       [9, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tf.meshgrid(list(range(10)), [0], indexing=\"ij\")\n",
    "res = tf.squeeze(tf.stack(res, axis=1), axis=2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambda_)\n",
    "lambda_ = tf.scatter_update(\n",
    "        ref=lambda_, \n",
    "        indices=res, \n",
    "        updates=tf.add(tf.reduce_sum(tf.multiply(phi[:,:,0], nw * 0.0), axis=0, keepdims=False), 0.0))\n",
    "print(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.scatter_update(ref=lambda_, indices=res, updates=upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.scatter_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation using TensorFlow (Batch VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Latent Dirichlet Allocation model:\n",
    "\n",
    " - $\\theta_{d=1,...,M} \\sim \\mathrm{Dir}_K(\\alpha)$\n",
    " - $\\beta_{k=1,...,K} \\sim \\mathrm{Dir}_V(\\eta)$\n",
    " - $z_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ K}(\\theta_d)$\n",
    " - $w_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ V}(\\beta_{z_{di}})$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    " - $V$ is a vocabulary size\n",
    " - $K$ is a number of topics\n",
    " - $\\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution (known and symetric)\n",
    " - $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions (known and symetric)\n",
    " - $\\theta_d$ is the topic distribution for document d\n",
    " - $\\beta_k$ is the word distribution for topic k\n",
    " - $z_{di}$ is the topic for the i-th word in document d\n",
    " - $w_{di}$ is a specific word from d-th document belonging to $V$\n",
    "\n",
    "In plate notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hustlin_erd](LDA_plate_notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume full factorial design:\n",
    "\n",
    "\n",
    "$$q(z, \\beta, \\theta) = \\prod_d \\prod_i q(z_{di}) \\times \\prod_d q(\\theta_d) \\times \\prod_k q(\\beta_k)$$\n",
    "\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $q(z_{di}) = p(z_{di}|\\phi) = \\mathrm{Multinomial}_{ \\ K}(\\phi_{dw_{di}}) $\n",
    "- $q(\\theta_d) = p(\\theta_d|\\gamma) = \\mathrm{Dir}_{K}(\\gamma_{d}) $\n",
    "- $q(\\beta_k) = p(\\beta_k|\\lambda) = \\mathrm{Dir}_{V}(\\lambda_{k}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vocabulary to topic membership:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi = \\phi_{d=1,...,D, \\ v=1,...,V, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times V \\times K}$$\n",
    "$ \\forall_{d \\in 1,...,D} \\ \\phi_d $ is a *stochastic matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distribution of topics:*\n",
    "$$ \\gamma = \\gamma_{d=1,...,D, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Within topic vocabulary profile:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\lambda = \\lambda_{k=1,...,K \\ v=1,...,V} \\in \\mathbb{R}_{+}^{K \\times V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of the **E**vidence **L**ower **Bo**und:\n",
    "\n",
    "$$ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) := \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log p(w, z, \\theta, \\beta| \\alpha, \\eta) \\right] - \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log q(z, \\theta, \\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probability factorization of the LDA model\n",
    "\n",
    "$$ \\log p(w, z, \\theta, \\beta| \\alpha, \\eta) = \\log p(w|z, \\beta) p(z|\\theta) p(\\theta | \\alpha) p(\\beta | \\eta) = $$\n",
    "\n",
    "$$ = \\log \\left( \\prod_{k=1}^K p(\\beta_k|\\eta) \\times \\prod_{d=1}^D p(\\theta_d|\\alpha) \\times \\prod_{d=1}^{D} \\prod_{i=1}^{N_d} p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\log p(\\theta_d|\\alpha) + \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log \\left( p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) + \\frac{1}{D} \\sum_{k=1}^K \\log p(\\beta_k|\\eta) \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the expectations with respect to the posterior distribution results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log p(\\theta_d|\\alpha) \\right] = C_1(\\alpha) + \\sum_{k=1}^K (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\beta} \\left[ \\log p(w_{di}|\\beta_{z_{di}}) \\right] =  \\mathbb{E}_{z, \\beta} \\left[ \\log \\beta_{z_{di} i} \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\theta} \\left[ \\log p(z_{di}|\\theta_d) \\right] =  \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log p(\\beta_{k}|\\eta) \\right] = C_2(\\eta) + \\sum_{v=1}^{V} (\\eta_v - 1) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entropy of the posterior distribution we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log\\left(\\prod_{d=1}^{D} \\prod_{i=1}^{N_{d}} q(z_{di}) \\times \\prod_{d=1}^D q(\\theta_d) \\times \\prod_{k=1}^{K} q(\\beta_k)\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\left( \\sum_{i=1}^{N_{d}} \\log q(z_{di}) + \\log q(\\theta_d) + \\frac{1}{D} \\sum_{k=1}^{K} \\log q(\\beta_k)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and expectations with respect to the posterior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z} \\left[ \\log q(z_{di})  \\right] = \\mathbb{E}_{z_{di}} \\left[ \\mathbb{1}_{[z_{di}=k]} \\log \\phi_{dw_{di}k}  \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\log \\phi_{dw_{di}k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log q(\\theta_d)  \\right] = \\mathbb{E}_{\\theta} \\left[ \\log \\frac{1}{B(\\gamma_d)} \\prod_{k=1}^{K} \\theta_{dk}^{\\gamma_{dk} - 1} \\right] = -\\log B(\\gamma_d) + \\sum_{k=1}^K (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta}\\left[ \\log q(\\beta_k) \\right] = -\\log B(\\lambda_k) + \\sum_{v=1}^V (\\lambda_{kv} - 1)\\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We should then be able to collapse ELBO to the formula based on counts of words rather than words from documents. This will show that the LDA model is fixed size method.* \n",
    "\n",
    "*\"When training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\"* (Data Camp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping terms by corresponding parameters we get:\n",
    "\n",
    "#### Document vocabulary membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\phi_{dv}]} = \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\sum_{k=1}^K \\phi_{dw_{di}k} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dw_{di}k}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n_{dv}$ is a number of times word $v$ from vocabulary $V$ was counted in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\gamma_{d}]} = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\log B(\\gamma_d) - (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{D} \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K \\left\\{ (\\eta_v - \\lambda_{kv}) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k} \\right] + \\frac{1}{V} \\log B(\\lambda_k) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate ascent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for every document and vocabulary entry separately we get:\n",
    "\n",
    "#### Document vocabulary membership:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\mathcal{L}_{[\\phi_{dv}]} =  \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ =\\mathrm{argmax}_{\\phi_{dv} \\in \\mathbb{R}_{+}^K} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) + \\xi (\\sum_{k=1}^K \\phi_{dvk} - 1) = \\Psi(\\phi_{dv}, \\xi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\xi} \\Psi(\\phi_{dv}, \\xi) = \\sum_{k=1}^K \\phi_{dvk} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = 0 \\iff n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 + \\xi' = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\log \\phi_{dvk} = \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - 1 + \\xi'  \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doucuments' distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$ \\log B(\\gamma_d) = \\log \\frac{\\prod_{k=1}^{K}\\Gamma(\\gamma_{dk})}{\\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})} = \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\mathcal{L}_{[\\gamma_{d}]} = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} + \\log B(\\gamma_d) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) (\\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}}) = \\Psi(\\gamma_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = -\\psi(\\gamma_{dj}) + \\psi({\\sum_k \\gamma_{dk})} + \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\psi(\\gamma_{dj}) - \\psi(\\sum_k \\gamma_{dk}) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *trigamma* function has no real roots it follows that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = 0 \\iff \\forall_{k} \\ \\ \\  \\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk} = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff   \\gamma_{dk} = \\alpha_k  + \\sum_{v=1}^V n_{dv} \\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile (M-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact derivation as for the $\\gamma_d$ gives an update equation for $\\lambda_k$\n",
    "\n",
    "$$ \\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - initialize $\\lambda$ and $\\gamma$\n",
    " - while improvement $ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) > 1e-6$ \n",
    "     - for $d = 1,...,D$\n",
    "         - $\\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\}$\n",
    "         - $\\gamma_{dk} = \\alpha  + \\sum_{v=1}^V n_{dv} \\phi_{dvk}$\n",
    "     - $\\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1448625, shape=(5, 4), dtype=float32, numpy=\n",
       "array([[19., 31., 33., 21.],\n",
       "       [ 4., 14., 15., 71.],\n",
       "       [ 4., 10., 10., 80.],\n",
       "       [10., 20., 23., 51.],\n",
       "       [ 7., 15., 11., 71.]], dtype=float32)>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set data dimensions\n",
    "K = 2\n",
    "V = 4\n",
    "D = 5\n",
    "N = 100\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(2014)\n",
    "\n",
    "# beta prior parameters\n",
    "eta = np.ones(V)\n",
    "\n",
    "# beta profiles\n",
    "beta = np.random.dirichlet(alpha=eta, size=K)\n",
    "\n",
    "# theta prior parameters\n",
    "alpha = np.ones(K)\n",
    "\n",
    "# document's prior topic allocation\n",
    "theta = np.random.dirichlet(alpha=alpha, size=D)\n",
    "\n",
    "# word's topic membership\n",
    "z = [np.random.choice(K, size=N, replace=True, p=theta[d, :]) for d in range(D)]\n",
    "z = np.vstack(z)\n",
    "\n",
    "# actual words and counts\n",
    "w = [np.array([np.random.choice(V, size=1, p=beta[k,:])[0] for k in z[d, :]]  + list(range(V))) for d in range(D)]\n",
    "nw = [np.unique(w[d], return_counts=True)[1] for d in range(D)]\n",
    "nw = np.vstack(nw)\n",
    "w = np.vstack(w)\n",
    "\n",
    "nw = tf.convert_to_tensor(nw, dtype=tf.float32)\n",
    "nw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model\n",
    "\n",
    "Few notes:\n",
    "- there is a tape now for gradients like in pyTorch\n",
    "- we will only use tensorflow for fast tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LDA parameters\n",
    "def initialize_parameters(K, V, D, alpha=1e-2, eta=1e-2, seed=2014):\n",
    "    \"\"\"\n",
    "    Initialize parameters of LDA model returning adequate Tensors.\n",
    "\n",
    "    args:\n",
    "    \n",
    "        K (int): number of LDA components \n",
    "        V (int): vocabulary size\n",
    "        D (int): number of documents\n",
    "        alpha (float): hyperparameter for theta prior\n",
    "        eta (float): hyperparameter for beta prior\n",
    "       \n",
    "       \n",
    "    returns:\n",
    "    \n",
    "        eta: [1 x V] tensor with prior parameters (alpha) for beta\n",
    "        lambda: k x [1, V] tensors with posterior word distribution per class\n",
    "        phi: [D, V, K] tensor with vocabulary membership per document\n",
    "        gamma: k x [D, 1] tensors\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    eta = tf.ones((1, V)) * alpha\n",
    "    lambda_ = [tf.abs(tf.random_normal(shape=(1, V), seed=seed)) for k in range(K)]\n",
    "    \n",
    "    phi = tf.random_normal(shape=(D, V, K), seed=seed)\n",
    "    phi = tf.nn.softmax(phi, axis=2)\n",
    "    \n",
    "    gamma = [tf.abs(tf.random_normal(shape=(1, V), seed=seed)) for k in range(K)]\n",
    "    \n",
    "    return eta, lambda_, phi\n",
    "\n",
    "eta, lambda_, phi = initialize_parameters(K, V, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=1448634, shape=(1, 4), dtype=float32, numpy=array([[0.02746824, 0.6522103 , 0.9192426 , 1.4331781 ]], dtype=float32)>,\n",
       " <tf.Tensor: id=1448639, shape=(1, 4), dtype=float32, numpy=array([[2.0289245 , 0.1510453 , 0.96964526, 0.11797091]], dtype=float32)>]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update for lambda\n",
    "def update_lambda(lambda_, eta, phi):\n",
    "    \n",
    "    K = len(lambda_)\n",
    "    for k in range(K):\n",
    "        lambda_[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=0, keepdims=True), eta)\n",
    "    return lambda_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_log_theta()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-7050.6655, shape=(), dtype=float32)\n",
      "tf.Tensor(8332.171, shape=(), dtype=float32)\n",
      "tf.Tensor(-782.49097, shape=(), dtype=float32)\n",
      "tf.Tensor(-18377.848, shape=(), dtype=float32)\n",
      "tf.Tensor(4282.1626, shape=(), dtype=float32)\n",
      "tf.Tensor(2693.7761, shape=(), dtype=float32)\n",
      "tf.Tensor(26654.258, shape=(), dtype=float32)\n",
      "tf.Tensor(6411.775, shape=(), dtype=float32)\n",
      "tf.Tensor(-11554.484, shape=(), dtype=float32)\n",
      "tf.Tensor(19046.424, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "K = 100\n",
    "V = 100000\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    b = tf.random_normal((K, V))\n",
    "    b = tf.transpose(b)\n",
    "    b = tf.expand_dims(input=b, axis=0)\n",
    "    b = tf.tile(b, multiples=[D, 1, 1])\n",
    "    print(tf.reduce_sum(b))\n",
    "\n",
    "# b_dvk = \n",
    "# tf.tile(tf.expand_dims(tf.transpose(b)), multiples=[D, 1, 1])\n",
    "# a = tf.random_normal((D, K))\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n",
       "\n",
       "Packs the list of tensors in `values` into a tensor with rank one higher than\n",
       "each tensor in `values`, by packing them along the `axis` dimension.\n",
       "Given a list of length `N` of tensors of shape `(A, B, C)`;\n",
       "\n",
       "if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n",
       "if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n",
       "Etc.\n",
       "\n",
       "For example:\n",
       "\n",
       "```python\n",
       "x = tf.constant([1, 4])\n",
       "y = tf.constant([2, 5])\n",
       "z = tf.constant([3, 6])\n",
       "tf.stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)\n",
       "tf.stack([x, y, z], axis=1)  # [[1, 2, 3], [4, 5, 6]]\n",
       "```\n",
       "\n",
       "This is the opposite of unstack.  The numpy equivalent is\n",
       "\n",
       "```python\n",
       "tf.stack([x, y, z]) = np.stack([x, y, z])\n",
       "```\n",
       "\n",
       "Args:\n",
       "  values: A list of `Tensor` objects with the same shape and type.\n",
       "  axis: An `int`. The axis to stack along. Defaults to the first dimension.\n",
       "    Negative values wrap around, so the valid range is `[-(R+1), R+1)`.\n",
       "  name: A name for this operation (optional).\n",
       "\n",
       "Returns:\n",
       "  output: A stacked `Tensor` with the same type as `values`.\n",
       "\n",
       "Raises:\n",
       "  ValueError: If `axis` is out of the range [-(R+1), R+1).\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/share/virtualenvs/LDA-pHQnYWLy/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tf.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

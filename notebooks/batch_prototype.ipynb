{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation using TensorFlow (Batch VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "v1.6.0-0-gd2e24b6039\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.__git_version__)\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=18, shape=(10, 2), dtype=int32, numpy=\n",
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [2, 1],\n",
       "       [3, 1],\n",
       "       [4, 1],\n",
       "       [5, 1],\n",
       "       [6, 1],\n",
       "       [7, 1],\n",
       "       [8, 1],\n",
       "       [9, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tf.meshgrid(list(range(10)), [1], indexing=\"ij\")\n",
    "res = tf.squeeze(tf.stack(res, axis=1), axis=2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(10, 3) dtype=float32, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]\n",
      " [ 1. -1.  1.]], shape=(10, 3), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(10, 3) dtype=float32, numpy=\n",
      "array([[ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.],\n",
      "       [ 1., -1.,  1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "ref = tf.fill(value=1.0, dims=(10, 3))\n",
    "ref = tfe.Variable(ref)\n",
    "upd = tf.fill(value=-1.0, dims=(10, ))\n",
    "print(ref)\n",
    "print(tf.scatter_nd_update(ref=ref, indices=res, updates=upd))\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Latent Dirichlet Allocation model:\n",
    "\n",
    " - $\\theta_{d=1,...,M} \\sim \\mathrm{Dir}_K(\\alpha)$\n",
    " - $\\beta_{k=1,...,K} \\sim \\mathrm{Dir}_V(\\eta)$\n",
    " - $z_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ K}(\\theta_d)$\n",
    " - $w_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ V}(\\beta_{z_{di}})$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    " - $V$ is a vocabulary size\n",
    " - $K$ is a number of topics\n",
    " - $\\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution (known and symetric)\n",
    " - $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions (known and symetric)\n",
    " - $\\theta_d$ is the topic distribution for document d\n",
    " - $\\beta_k$ is the word distribution for topic k\n",
    " - $z_{di}$ is the topic for the i-th word in document d\n",
    " - $w_{di}$ is a specific word from d-th document belonging to $V$\n",
    "\n",
    "In plate notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hustlin_erd](LDA_plate_notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume full factorial design:\n",
    "\n",
    "\n",
    "$$q(z, \\beta, \\theta) = \\prod_d \\prod_i q(z_{di}) \\times \\prod_d q(\\theta_d) \\times \\prod_k q(\\beta_k)$$\n",
    "\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $q(z_{di}) = p(z_{di}|\\phi) = \\mathrm{Multinomial}_{ \\ K}(\\phi_{dw_{di}}) $\n",
    "- $q(\\theta_d) = p(\\theta_d|\\gamma) = \\mathrm{Dir}_{K}(\\gamma_{d}) $\n",
    "- $q(\\beta_k) = p(\\beta_k|\\lambda) = \\mathrm{Dir}_{V}(\\lambda_{k}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vocabulary to topic membership:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi = \\phi_{d=1,...,D, \\ v=1,...,V, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times V \\times K}$$\n",
    "$ \\forall_{d \\in 1,...,D} \\ \\phi_d $ is a *stochastic matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distribution of topics:*\n",
    "$$ \\gamma = \\gamma_{d=1,...,D, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Within topic vocabulary profile:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\lambda = \\lambda_{k=1,...,K \\ v=1,...,V} \\in \\mathbb{R}_{+}^{K \\times V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of the **E**vidence **L**ower **Bo**und:\n",
    "\n",
    "$$ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) := \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log p(w, z, \\theta, \\beta| \\alpha, \\eta) \\right] - \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log q(z, \\theta, \\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probability factorization of the LDA model\n",
    "\n",
    "$$ \\log p(w, z, \\theta, \\beta| \\alpha, \\eta) = \\log p(w|z, \\beta) p(z|\\theta) p(\\theta | \\alpha) p(\\beta | \\eta) = $$\n",
    "\n",
    "$$ = \\log \\left( \\prod_{k=1}^K p(\\beta_k|\\eta) \\times \\prod_{d=1}^D p(\\theta_d|\\alpha) \\times \\prod_{d=1}^{D} \\prod_{i=1}^{N_d} p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\log p(\\theta_d|\\alpha) + \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log \\left( p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) + \\frac{1}{D} \\sum_{k=1}^K \\log p(\\beta_k|\\eta) \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the expectations with respect to the posterior distribution results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log p(\\theta_d|\\alpha) \\right] = C_1(\\alpha) + \\sum_{k=1}^K (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\beta} \\left[ \\log p(w_{di}|\\beta_{z_{di}}) \\right] =  \\mathbb{E}_{z, \\beta} \\left[ \\log \\beta_{z_{di} i} \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\theta} \\left[ \\log p(z_{di}|\\theta_d) \\right] =  \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log p(\\beta_{k}|\\eta) \\right] = C_2(\\eta) + \\sum_{v=1}^{V} (\\eta_v - 1) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entropy of the posterior distribution we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log\\left(\\prod_{d=1}^{D} \\prod_{i=1}^{N_{d}} q(z_{di}) \\times \\prod_{d=1}^D q(\\theta_d) \\times \\prod_{k=1}^{K} q(\\beta_k)\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\left( \\sum_{i=1}^{N_{d}} \\log q(z_{di}) + \\log q(\\theta_d) + \\frac{1}{D} \\sum_{k=1}^{K} \\log q(\\beta_k)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and expectations with respect to the posterior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z} \\left[ \\log q(z_{di})  \\right] = \\mathbb{E}_{z_{di}} \\left[ \\mathbb{1}_{[z_{di}=k]} \\log \\phi_{dw_{di}k}  \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\log \\phi_{dw_{di}k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log q(\\theta_d)  \\right] = \\mathbb{E}_{\\theta} \\left[ \\log \\frac{1}{B(\\gamma_d)} \\prod_{k=1}^{K} \\theta_{dk}^{\\gamma_{dk} - 1} \\right] = -\\log B(\\gamma_d) + \\sum_{k=1}^K (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta}\\left[ \\log q(\\beta_k) \\right] = -\\log B(\\lambda_k) + \\sum_{v=1}^V (\\lambda_{kv} - 1)\\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We should then be able to collapse ELBO to the formula based on counts of words rather than words from documents. This will show that the LDA model is fixed size method.* \n",
    "\n",
    "*\"When training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\"* (Data Camp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping terms by corresponding parameters we get:\n",
    "\n",
    "#### Document vocabulary membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\phi_{dv}]} = \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\sum_{k=1}^K \\phi_{dw_{di}k} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dw_{di}k}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n_{dv}$ is a number of times word $v$ from vocabulary $V$ was counted in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\gamma_{d}]} = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\log B(\\gamma_d) - (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{D} \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K \\left\\{ (\\eta_v - \\lambda_{kv}) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k} \\right] + \\frac{1}{V} \\log B(\\lambda_k) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate ascent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for every document and vocabulary entry separately we get:\n",
    "\n",
    "#### Document vocabulary membership:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\mathcal{L}_{[\\phi_{dv}]} =  \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ =\\mathrm{argmax}_{\\phi_{dv} \\in \\mathbb{R}_{+}^K} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) + \\xi (\\sum_{k=1}^K \\phi_{dvk} - 1) = \\Psi(\\phi_{dv}, \\xi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\xi} \\Psi(\\phi_{dv}, \\xi) = \\sum_{k=1}^K \\phi_{dvk} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = 0 \\iff n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 + \\xi' = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\log \\phi_{dvk} = \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - 1 + \\xi'  \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doucuments' distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$ \\log B(\\gamma_d) = \\log \\frac{\\prod_{k=1}^{K}\\Gamma(\\gamma_{dk})}{\\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})} = \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\mathcal{L}_{[\\gamma_{d}]} = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} + \\log B(\\gamma_d) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) (\\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}}) = \\Psi(\\gamma_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = -\\psi(\\gamma_{dj}) + \\psi({\\sum_k \\gamma_{dk})} + \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\psi(\\gamma_{dj}) - \\psi(\\sum_k \\gamma_{dk}) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *trigamma* function has no real roots it follows that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = 0 \\iff \\forall_{k} \\ \\ \\  \\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk} = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff   \\gamma_{dk} = \\alpha_k  + \\sum_{v=1}^V n_{dv} \\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile (M-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact derivation as for the $\\gamma_d$ gives an update equation for $\\lambda_k$\n",
    "\n",
    "$$ \\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - initialize $\\lambda$ and $\\gamma$\n",
    " - while improvement $ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) > 1e-6$ \n",
    "     - for $d = 1,...,D$\n",
    "         - $\\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\}$\n",
    "         - $\\gamma_{dk} = \\alpha  + \\sum_{v=1}^V n_{dv} \\phi_{dvk}$\n",
    "     - $\\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data dimensions\n",
    "K = 3\n",
    "V = 10\n",
    "D = 1000\n",
    "N = 100\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(2014)\n",
    "\n",
    "# beta prior parameters\n",
    "eta = np.ones(V) * 1e-1\n",
    "\n",
    "# beta profiles\n",
    "beta = np.random.dirichlet(alpha=eta, size=K)\n",
    "\n",
    "# theta prior parameters\n",
    "alpha = np.ones(K) * 1e-1\n",
    "# alpha[0] = 10\n",
    "\n",
    "# document's prior topic allocation\n",
    "theta = np.random.dirichlet(alpha=alpha, size=D)\n",
    "\n",
    "# word's topic membership\n",
    "z = [np.random.choice(K, size=N, replace=True, p=theta[d, :]) for d in range(D)]\n",
    "z = np.vstack(z)\n",
    "\n",
    "# actual words and counts\n",
    "w = [np.array([np.random.choice(V, size=1, p=beta[k,:])[0] for k in z[d, :]]  + list(range(V))) for d in range(D)]\n",
    "nw = [np.unique(w[d], return_counts=True)[1] for d in range(D)]\n",
    "nw = np.vstack(nw)\n",
    "w = np.vstack(w)\n",
    "\n",
    "nw = tf.convert_to_tensor(nw, dtype=tf.float32)\n",
    "# nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model\n",
    "\n",
    "Few notes:\n",
    "- there is a tape now for gradients like in pyTorch\n",
    "- we will only use tensorflow for fast tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=47, shape=(1, 10), dtype=float32, numpy=array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize LDA parameters\n",
    "def initialize_parameters(K, V, D, alpha=1e-1, eta=1e-1, seed=2014):\n",
    "    \"\"\"\n",
    "    Initialize parameters of LDA model returning adequate Tensors.\n",
    "\n",
    "    args:\n",
    "    \n",
    "        K (int): number of LDA components \n",
    "        V (int): vocabulary size\n",
    "        D (int): number of documents\n",
    "        alpha (float): hyperparameter for theta prior\n",
    "        eta (float): hyperparameter for beta prior\n",
    "       \n",
    "       \n",
    "    returns:\n",
    "    \n",
    "        eta: [1 x V] tensor with prior parameters (alpha) for beta\n",
    "        lambda: k x [1, V] tensors with posterior word distribution per class\n",
    "        phi: [D, V, K] tensor with vocabulary membership per document\n",
    "        gamma: k x [D, 1] tensors\n",
    "        \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    eta = tf.ones((1, V)) * eta\n",
    "    alpha = tf.ones((K, 1)) * alpha\n",
    "    \n",
    "    lambda_ = [tf.abs(tf.random_normal(shape=(1, V))) for k in range(K)]\n",
    "    \n",
    "    phi = tf.random_normal(shape=(D, V, K))\n",
    "    phi = tf.nn.softmax(phi, axis=2)\n",
    "    \n",
    "    gamma = [tf.abs(tf.random_normal(shape=(D, 1))) for k in range(K)]\n",
    "    \n",
    "    return eta, alpha, lambda_, phi, gamma\n",
    "\n",
    "# test\n",
    "eta, alpha, lambda_, phi, gamma = initialize_parameters(K, V, D)\n",
    "eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda update\n",
    "def update_lambda(lambda_, eta, phi, nw):\n",
    "    \n",
    "    K = len(lambda_)\n",
    "    for k in range(K):\n",
    "        lambda_[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=0, keepdims=True), eta)\n",
    "        \n",
    "    return lambda_ \n",
    "\n",
    "# test\n",
    "# print(lambda_)\n",
    "# update_lambda(lambda_, eta, phi, nw)\n",
    "# print(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma update\n",
    "def update_gamma(gamma, alpha, phi, nw):\n",
    "    \n",
    "    K = len(gamma)\n",
    "    for k in range(K):\n",
    "        gamma[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=1, keepdims=True), alpha[k])\n",
    "        \n",
    "    return gamma\n",
    "\n",
    "# test\n",
    "# print(gamma)\n",
    "# update_gamma(gamma, alpha, phi, nw)\n",
    "# print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right]  = \\psi(\\lambda_{kv}) - \\psi(\\sum_{v=1}^V \\lambda_{kv}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_log_beta(lambda_):\n",
    "    return [tf.digamma(lam) - tf.digamma(tf.reduce_sum(lam)) for lam in lambda_]\n",
    "\n",
    "# e_log_beta(lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_log_theta(gamma):\n",
    "    return [tf.digamma(g_k) - tf.digamma(tf.reduce_sum(g_k)) for g_k in gamma]\n",
    "\n",
    "# e_log_theta(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi(lambda_, gamma, D, V):\n",
    "    eb_1xvxk = tf.stack(e_log_beta(lambda_), axis=2)\n",
    "    eb_dxvxk = tf.tile(eb_1xvxk, multiples=[D, 1, 1])\n",
    "    et_dx1xk = tf.stack(e_log_theta(gamma), axis=2)\n",
    "    et_dxvxk = tf.tile(et_dx1xk, multiples=[1, V, 1])\n",
    "    phi_dxvxk = tf.nn.softmax(logits=eb_dxvxk + et_dxvxk, axis=2)\n",
    "    return phi_dxvxk\n",
    "\n",
    "# update_phi(lambda_, gamma, D, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A := \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ B := \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.225006"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elbo(lambda_, gamma, D, V, nw):\n",
    "    eb_1xvxk = tf.stack(e_log_beta(lambda_), axis=2)\n",
    "    eb_dxvxk = tf.tile(eb_1xvxk, multiples=[D, 1, 1])\n",
    "    et_dx1xk = tf.stack(e_log_theta(gamma), axis=2)\n",
    "    et_dxvxk = tf.tile(et_dx1xk, multiples=[1, V, 1])\n",
    "    phi_dxvxk = tf.nn.softmax(logits=eb_dxvxk + et_dxvxk, axis=2)\n",
    "    n_dxvxk = tf.tile(tf.expand_dims(nw / tf.reduce_sum(nw), axis=2), multiples=[1, 1, K])\n",
    "    \n",
    "    A = tf.reduce_sum(n_dxvxk * phi_dxvxk * (eb_dxvxk + et_dxvxk - tf.log(phi_dxvxk + 1e-6)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return A.numpy()\n",
    "\n",
    "elbo(lambda_, gamma, D, V, nw)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it all up together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ELBO: -7.61189\n",
      "Iteration: 1 ELBO: -7.5102015\n",
      "Iteration: 2 ELBO: -7.4390783\n",
      "Iteration: 3 ELBO: -7.385777\n",
      "Iteration: 4 ELBO: -7.347884\n",
      "Iteration: 5 ELBO: -7.323457\n",
      "Iteration: 6 ELBO: -7.308981\n",
      "Iteration: 7 ELBO: -7.301273\n",
      "Iteration: 8 ELBO: -7.297392\n",
      "Iteration: 9 ELBO: -7.295474\n",
      "Iteration: 10 ELBO: -7.2944207\n",
      "Iteration: 11 ELBO: -7.293814\n",
      "Iteration: 12 ELBO: -7.293473\n",
      "Iteration: 13 ELBO: -7.2932143\n",
      "Iteration: 14 ELBO: -7.292981\n",
      "Iteration: 15 ELBO: -7.2927957\n",
      "Iteration: 16 ELBO: -7.2926826\n",
      "Iteration: 17 ELBO: -7.2926016\n",
      "Iteration: 18 ELBO: -7.292543\n",
      "Iteration: 19 ELBO: -7.2924767\n",
      "Iteration: 20 ELBO: -7.292369\n",
      "Iteration: 21 ELBO: -7.292183\n",
      "Iteration: 22 ELBO: -7.292083\n",
      "Iteration: 23 ELBO: -7.2920647\n",
      "Iteration: 24 ELBO: -7.2920504\n",
      "Iteration: 25 ELBO: -7.2920423\n",
      "Iteration: 26 ELBO: -7.292046\n",
      "Iteration: 27 ELBO: -7.2920423\n",
      "Iteration: 28 ELBO: -7.292036\n",
      "Iteration: 29 ELBO: -7.292025\n",
      "Iteration: 30 ELBO: -7.291994\n",
      "Iteration: 31 ELBO: -7.291955\n",
      "Iteration: 32 ELBO: -7.2919455\n",
      "Iteration: 33 ELBO: -7.291953\n",
      "Iteration: 34 ELBO: -7.29195\n",
      "Iteration: 35 ELBO: -7.291952\n",
      "Iteration: 36 ELBO: -7.2919493\n",
      "Iteration: 37 ELBO: -7.2919407\n",
      "Iteration: 38 ELBO: -7.291957\n",
      "Iteration: 39 ELBO: -7.2919445\n",
      "Iteration: 40 ELBO: -7.291935\n",
      "Iteration: 41 ELBO: -7.2919235\n",
      "Iteration: 42 ELBO: -7.2919226\n",
      "Converged!\n"
     ]
    }
   ],
   "source": [
    "seed += 1\n",
    "eta, alpha, lambda_, phi, gamma = initialize_parameters(K, V, D, seed=seed)\n",
    "\n",
    "prev_elbo = 0.0\n",
    "next_elbo = 0.0\n",
    "iter = 0\n",
    "\n",
    "for i in range(100000):\n",
    "    \n",
    "    for j in range(100000):\n",
    "        # E-Step:\n",
    "        phi = update_phi(lambda_, gamma, D, V)\n",
    "        gamma_next = update_gamma(gamma, alpha, phi, nw)\n",
    "        \n",
    "        diff = np.mean([tf.reduce_sum(tf.abs(g_prev - g_next)).numpy() for g_prev, g_next in zip(gamma, gamma_next)])\n",
    "        gamma = gamma_next\n",
    "        if diff < 1e-6:\n",
    "            break\n",
    "    \n",
    "    # M-Step:\n",
    "    lambda_ = update_lambda(lambda_, eta, phi, nw)\n",
    "    \n",
    "    \n",
    "    next_elbo = elbo(lambda_, gamma, D, V, nw)\n",
    "    print(\"Iteration:\", iter, \"ELBO:\", next_elbo)\n",
    "    \n",
    "    diff = np.abs(next_elbo - prev_elbo)\n",
    "    if diff < 1e-6:\n",
    "        print(\"Converged!\")\n",
    "        break\n",
    "    else:\n",
    "        iter += 1\n",
    "        prev_elbo = next_elbo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.063 0.    0.927 0.001 0.008 0.    0.   ]\n",
      " [0.579 0.01  0.    0.003 0.033 0.006 0.363 0.    0.    0.006]\n",
      " [0.041 0.    0.    0.001 0.46  0.079 0.    0.086 0.333 0.   ]]\n",
      "[[0.487 0.017 0.01  0.019 0.    0.126 0.315 0.009 0.004 0.015]\n",
      " [0.011 0.009 0.009 0.06  0.064 0.757 0.009 0.026 0.045 0.009]\n",
      " [0.121 0.01  0.009 0.011 0.38  0.07  0.061 0.072 0.255 0.01 ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(beta, decimals=3))\n",
    "print(np.round(np.vstack([(lam / tf.reduce_sum(lam)).numpy()  for lam in lambda_]), decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate results for visualisation of topics\n",
    "\n",
    "https://pyldavis.readthedocs.io/en/latest/modules/API.html#pyLDAvis.prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic term distribution:\n",
    "topic_term_dist = np.round(np.vstack([(lam / tf.reduce_sum(lam)).numpy()  for lam in lambda_]), decimals=3)\n",
    "topic_term_dist\n",
    "\n",
    "# doc_topic_dists :array-like, shape (n_docs, n_topics)\n",
    "doc_topic_dist = tf.stack([tf.reshape(g_k, shape=(1000, )) for g_k in gamma], axis=1)\n",
    "doc_topic_dist = doc_topic_dist / tf.reduce_sum(doc_topic_dist, axis=1, keep_dims=True)\n",
    "doc_topic_dist = doc_topic_dist.numpy()\n",
    "\n",
    "# doc_lengths :array-like, shape n_docs\n",
    "doc_len = tf.reduce_sum(nw, axis=1)\n",
    "doc_len = doc_len.numpy()\n",
    "\n",
    "# vocab :array-like, shape n_terms\n",
    "vocab = np.array(list(range(V)))\n",
    "\n",
    "# term_frequency :array-like, shape n_terms\n",
    "term_frec = tf.reduce_sum(nw, axis=0)\n",
    "term_frec = term_frec.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qba/.local/share/virtualenvs/LDA-pHQnYWLy/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "topics = pyLDAvis.prepare(topic_term_dist, doc_topic_dist, doc_len, vocab, term_frec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(topics, fileobj=\"results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "The dataset is in the form of a 11463 x 5812 matrix of word counts, containing 11463 words and 5811 NIPS conference papers (the first column contains the list of words). Each column contains the number of times each word appears in the corresponding document. The names of the columns give information about each document and its timestamp in the following format: Xyear_paperID. \n",
    "\n",
    "The matrix of word counts was obtained using the R package 'tmâ€ to process the raw .txt files of the full text of the NIPS conference papers published between 1987 and 2015. The document-term matrix was constructed after tokenization, removal of stopwords and truncation of the vocabulary by only keeping words occurring more than 50 times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  127M  100  127M    0     0  17.2M      0  0:00:07  0:00:07 --:--:-- 22.9M\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "! curl https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv > NIPS_1987-2015.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"NIPS_1987-2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = np.array(data.iloc[:, 1:])\n",
    "nw = nw.astype('float32')\n",
    "nw = nw.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = tf.convert_to_tensor(nw)\n",
    "nw = nw[0:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "D, V = nw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "eta, alpha, lambda_, phi, gamma = initialize_parameters(K, V, D, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ELBO: -12.418982\n",
      "12.02796196937561\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "def lda(eta=eta, alpha=alpha, lambda_=lambda_, phi=phi, gamma=gamma, K=K, V=V, D=D, seed=seed):\n",
    "\n",
    "    prev_elbo = 0.0\n",
    "    next_elbo = 0.0\n",
    "    iter = 0\n",
    "\n",
    "    for i in range(1):\n",
    "\n",
    "        for j in range(1):\n",
    "            # E-Step:\n",
    "            phi = update_phi(lambda_, gamma, D, V)\n",
    "            gamma_next = update_gamma(gamma, alpha, phi, nw)\n",
    "\n",
    "            diff = 0.0\n",
    "            diff = np.mean([tf.reduce_sum(tf.abs(g_prev - g_next)).numpy() for g_prev, g_next in zip(gamma, gamma_next)])\n",
    "            gamma = gamma_next\n",
    "            if diff < 1e-6:\n",
    "                break\n",
    "\n",
    "        # M-Step:\n",
    "        lambda_ = update_lambda(lambda_, eta, phi, nw)\n",
    "\n",
    "\n",
    "        next_elbo = elbo(lambda_, gamma, D, V, nw)\n",
    "        print(\"Iteration:\", iter, \"ELBO:\", next_elbo)\n",
    "\n",
    "        diff = np.abs(next_elbo - prev_elbo)\n",
    "        if diff < 1e-6:\n",
    "            print(\"Converged!\")\n",
    "            break\n",
    "        else:\n",
    "            iter += 1\n",
    "            prev_elbo = next_elbo\n",
    "\n",
    "        \n",
    "lda()            \n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ELBO: -12.421164\n",
      "         16394 function calls (16392 primitive calls) in 12.195 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      319   11.713    0.037   11.713    0.037 {built-in method _pywrap_tensorflow_internal.TFE_Py_Execute}\n",
      "        1    0.190    0.190    7.068    7.068 <ipython-input-12-23890523edae>:1(elbo)\n",
      "        1    0.122    0.122   12.176   12.176 <ipython-input-26-0e2d6dc2d4aa>:3(lda)\n",
      "        1    0.056    0.056    0.582    0.582 <ipython-input-8-371989c98d7e>:2(update_gamma)\n",
      "        1    0.056    0.056    0.602    0.602 <ipython-input-7-62df51f3fab9>:2(update_lambda)\n",
      "        1    0.020    0.020    3.800    3.800 <ipython-input-11-b39e0c75cc23>:1(update_phi)\n",
      "        1    0.019    0.019   12.195   12.195 <string>:1(<module>)\n",
      "      179    0.001    0.000    0.002    0.000 constant_op.py:89(convert_to_eager_tensor)\n",
      "     3592    0.001    0.000    0.001    0.000 {built-in method builtins.isinstance}\n",
      "      208    0.001    0.000    0.003    0.000 ops.py:953(internal_convert_to_tensor)\n",
      "  435/433    0.001    0.000    0.002    0.000 execute.py:166(args_to_matching_eager)\n",
      "      319    0.001    0.000   11.714    0.037 execute.py:33(quick_execute)\n",
      "       30    0.001    0.000    0.758    0.025 array_ops.py:458(_slice_helper)\n",
      "       58    0.001    0.000    0.924    0.016 math_ops.py:921(binary_op_wrapper)\n",
      "       72    0.001    0.000    0.100    0.001 gen_math_ops.py:4836(_sum)\n",
      "      184    0.000    0.000    0.002    0.000 tensor_shape.py:423(__init__)\n",
      "      319    0.000    0.000    0.001    0.000 backprop.py:241(_record_gradient)\n",
      "       72    0.000    0.000    0.104    0.001 math_ops.py:1315(reduce_sum)\n",
      "       30    0.000    0.000    0.754    0.025 gen_array_ops.py:5236(strided_slice)\n",
      "      270    0.000    0.000    0.000    0.000 tensor_shape.py:29(__init__)\n",
      "      184    0.000    0.000    0.002    0.000 ops.py:783(shape)\n",
      "      107    0.000    0.000    0.000    0.000 ops.py:5580(__enter__)\n",
      "      619    0.000    0.000    0.000    0.000 context.py:241(in_graph_mode)\n",
      "       80    0.000    0.000    0.009    0.000 gen_math_ops.py:1404(digamma)\n",
      "      107    0.000    0.000    0.000    0.000 ops.py:5566(__init__)\n",
      "       22    0.000    0.000    0.512    0.023 gen_math_ops.py:2772(_mul)\n",
      "       72    0.000    0.000    0.002    0.000 math_ops.py:1284(_ReductionDims)\n",
      "      319    0.000    0.000    0.001    0.000 tape.py:90(could_possibly_record)\n",
      "      179    0.000    0.000    0.002    0.000 constant_op.py:134(constant)\n",
      "      319    0.000    0.000    0.000    0.000 {built-in method _pywrap_tensorflow_internal.TFE_Py_TapeSetIsEmpty}\n",
      "      270    0.000    0.000    0.001    0.000 tensor_shape.py:383(as_dimension)\n",
      "      244    0.000    0.000    0.000    0.000 ops.py:663(dtype)\n",
      "      905    0.000    0.000    0.000    0.000 context.py:469(context)\n",
      "       74    0.000    0.000    2.103    0.028 deprecation.py:398(new_func)\n",
      "       24    0.000    0.000    0.488    0.020 gen_math_ops.py:165(add)\n",
      "      121    0.000    0.000    0.000    0.000 context.py:499(in_graph_mode)\n",
      "       53    0.000    0.000    0.152    0.003 gen_math_ops.py:4802(_sub)\n",
      "       30    0.000    0.000    0.755    0.025 array_ops.py:645(strided_slice)\n",
      "        2    0.000    0.000    0.011    0.006 <ipython-input-9-fffc08e77f94>:2(<listcomp>)\n",
      "      101    0.000    0.000    0.000    0.000 array_ops.py:988(_get_dtype_from_nested_lists)\n",
      "       72    0.000    0.000    0.001    0.000 math_ops.py:1307(_may_reduce_to_scalar)\n",
      "      107    0.000    0.000    0.000    0.000 ops.py:5626(__exit__)\n",
      "      152    0.000    0.000    0.000    0.000 dtypes.py:269(__eq__)\n",
      "      315    0.000    0.000    0.000    0.000 context.py:245(in_eager_mode)\n",
      "      184    0.000    0.000    0.001    0.000 tensor_shape.py:458(<listcomp>)\n",
      "      498    0.000    0.000    0.000    0.000 context.py:273(device_name)\n",
      "      168    0.000    0.000    0.002    0.000 ops.py:891(convert_to_tensor)\n",
      "      509    0.000    0.000    0.000    0.000 context.py:199(_handle)\n",
      "       52    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.arange}\n",
      "      645    0.000    0.000    0.000    0.000 {method '_datatype_enum' of 'EagerTensor' objects}\n",
      "       72    0.000    0.000    0.000    0.000 tensor_shape.py:788(is_fully_defined)\n",
      "        2    0.000    0.000    0.003    0.001 <ipython-input-10-ab8b2b872258>:2(<listcomp>)\n",
      "      156    0.000    0.000    0.000    0.000 execute.py:96(make_int)\n",
      "      125    0.000    0.000    0.001    0.000 constant_op.py:232(_constant_tensor_conversion_function)\n",
      "      214    0.000    0.000    0.000    0.000 context.py:258(scope_name)\n",
      "       94    0.000    0.000    0.003    0.000 array_ops.py:885(stack)\n",
      "      141    0.000    0.000    0.000    0.000 dtypes.py:102(base_dtype)\n",
      "       20    0.000    0.000    0.237    0.012 math_ops.py:326(multiply)\n",
      "      155    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "      101    0.000    0.000    0.001    0.000 array_ops.py:1009(_autopacking_conversion_function)\n",
      "       72    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        5    0.000    0.000    7.165    1.433 gen_array_ops.py:5565(tile)\n",
      "      198    0.000    0.000    0.000    0.000 dtypes.py:650(as_dtype)\n",
      "      293    0.000    0.000    0.000    0.000 dtypes.py:134(as_datatype_enum)\n",
      "        9    0.000    0.000    0.000    0.000 socket.py:333(send)\n",
      "      184    0.000    0.000    0.000    0.000 {method '_shape_tuple' of 'EagerTensor' objects}\n",
      "      235    0.000    0.000    0.000    0.000 ops.py:125(is_dense_tensor_like)\n",
      "      112    0.000    0.000    0.001    0.000 ops.py:787(get_shape)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:366(write)\n",
      "       72    0.000    0.000    0.000    0.000 execute.py:114(make_bool)\n",
      "        1    0.000    0.000    0.001    0.001 <ipython-input-26-0e2d6dc2d4aa>:17(<listcomp>)\n",
      "        1    0.000    0.000    0.532    0.532 gen_math_ops.py:2310(log)\n",
      "      184    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "      112    0.000    0.000    0.000    0.000 tensor_shape.py:790(<genexpr>)\n",
      "       10    0.000    0.000    0.000    0.000 math_ops.py:239(abs)\n",
      "        6    0.000    0.000    0.002    0.000 gen_array_ops.py:2794(_pack)\n",
      "      120    0.000    0.000    0.000    0.000 tensor_shape.py:476(ndims)\n",
      "      246    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "      141    0.000    0.000    0.000    0.000 dtypes.py:89(_is_ref_dtype)\n",
      "      107    0.000    0.000    0.000    0.000 context.py:253(scope_name)\n",
      "       11    0.000    0.000    0.000    0.000 ops.py:760(_copy)\n",
      "       11    0.000    0.000    0.000    0.000 {method '_copy_to_device' of 'EagerTensor' objects}\n",
      "       31    0.000    0.000    0.000    0.000 dtypes.py:279(__ne__)\n",
      "        4    0.000    0.000    0.000    0.000 gen_array_ops.py:3830(reshape)\n",
      "        2    0.000    0.000    1.998    0.999 nn_ops.py:1638(_softmax)\n",
      "      132    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        9    0.000    0.000    0.000    0.000 iostream.py:195(schedule)\n",
      "       11    0.000    0.000    0.000    0.000 {method '_numpy' of 'EagerTensor' objects}\n",
      "       11    0.000    0.000    0.000    0.000 ops.py:669(numpy)\n",
      "        4    0.000    0.000    0.000    0.000 array_ops.py:288(shape_internal)\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:53(_mean)\n",
      "        4    0.000    0.000    0.000    0.000 gen_array_ops.py:4435(shape)\n",
      "       10    0.000    0.000    0.000    0.000 gen_math_ops.py:24(_abs)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:943(_autopacking_helper)\n",
      "        2    0.000    0.000    0.001    0.000 nn_ops.py:1611(_flatten_outer_dims)\n",
      "        2    0.000    0.000    1.997    0.999 gen_nn_ops.py:4511(_softmax)\n",
      "        2    0.000    0.000    1.998    0.999 nn_ops.py:1715(softmax)\n",
      "        1    0.000    0.000   12.195   12.195 {built-in method builtins.exec}\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:1104(is_alive)\n",
      "       74    0.000    0.000    0.000    0.000 deprecation.py:504(deprecated_argument_lookup)\n",
      "        1    0.000    0.000    0.009    0.009 math_ops.py:1014(_truediv_python3)\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:434(rank_internal)\n",
      "        2    0.000    0.000    0.000    0.000 gen_array_ops.py:602(_concat_v2)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "       10    0.000    0.000    0.000    0.000 dtypes.py:157(is_complex)\n",
      "        7    0.000    0.000    0.000    0.000 dtypes.py:246(is_compatible_with)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:300(_is_master_process)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2854(mean)\n",
      "       28    0.000    0.000    0.000    0.000 context.py:249(scalar_cache)\n",
      "       11    0.000    0.000    0.000    0.000 tape.py:79(record_operation)\n",
      "       40    0.000    0.000    0.000    0.000 tensor_shape.py:80(value)\n",
      "        1    0.000    0.000    0.000    0.000 gen_array_ops.py:1185(_expand_dims)\n",
      "       11    0.000    0.000    0.000    0.000 ops.py:800(cpu)\n",
      "        2    0.000    0.000    0.000    0.000 gen_array_ops.py:4563(_slice)\n",
      "        9    0.000    0.000    0.000    0.000 iostream.py:93(_event_pipe)\n",
      "        4    0.000    0.000    0.000    0.000 execute.py:121(make_type)\n",
      "        1    0.000    0.000    0.009    0.009 gen_math_ops.py:3371(_real_div)\n",
      "        2    0.000    0.000    0.275    0.137 math_ops.py:1157(_mul_dispatch)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        1    0.000    0.000    0.000    0.000 _methods.py:43(_count_reduce_items)\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:592(slice)\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:1081(concat)\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:1062(_wait_for_tstate_lock)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:313(_schedule_flush)\n",
      "        2    0.000    0.000    0.003    0.001 <ipython-input-10-ab8b2b872258>:1(e_log_theta)\n",
      "        9    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 array_ops.py:401(rank)\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _pywrap_tensorflow_internal.TFE_Py_TapeSetRecordOperation}\n",
      "        4    0.000    0.000    0.000    0.000 execute.py:195(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:495(asanyarray)\n",
      "        4    0.000    0.000    0.000    0.000 array_ops.py:262(shape)\n",
      "        1    0.000    0.000    0.000    0.000 array_ops.py:142(expand_dims)\n",
      "        2    0.000    0.000    0.000    0.000 math_ops.py:346(subtract)\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:506(is_set)\n",
      "        4    0.000    0.000    0.000    0.000 tensor_shape.py:68(__int__)\n",
      "        2    0.000    0.000    0.011    0.006 <ipython-input-9-fffc08e77f94>:1(e_log_beta)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:297(__hash__)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prof = cProfile.run('lda()', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic term distribution:\n",
    "topic_term_dist = np.vstack([(lam / tf.reduce_sum(lam)).numpy()  for lam in lambda_])\n",
    "topic_term_dist\n",
    "\n",
    "# doc_topic_dists :array-like, shape (n_docs, n_topics)\n",
    "doc_topic_dist = tf.stack([tf.reshape(g_k, shape=(g_k.shape[0], )) for g_k in gamma], axis=1)\n",
    "doc_topic_dist = doc_topic_dist / tf.reduce_sum(doc_topic_dist, axis=1, keep_dims=True)\n",
    "doc_topic_dist = doc_topic_dist.numpy()\n",
    "\n",
    "# doc_lengths :array-like, shape n_docs\n",
    "doc_len = tf.reduce_sum(nw, axis=1)\n",
    "doc_len = doc_len.numpy()\n",
    "\n",
    "# vocab :array-like, shape n_terms\n",
    "vocab = data.iloc[:,0]\n",
    "\n",
    "# term_frequency :array-like, shape n_terms\n",
    "term_frec = tf.reduce_sum(nw, axis=0)\n",
    "term_frec = term_frec.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qba/.local/share/virtualenvs/LDA-pHQnYWLy/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "topics = pyLDAvis.prepare(topic_term_dist, doc_topic_dist, doc_len, vocab, term_frec)\n",
    "pyLDAvis.save_html(topics, fileobj=\"results.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_term_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite using updates instead of stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>\n",
      "tf.Tensor([-1. -1. -1.  1.  1.  1.  1.  1.  1.  1.], shape=(10,), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=array([-1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "ref = tf.fill(value=1.0, dims=(10, ))\n",
    "ref = tfe.Variable(ref)\n",
    "upd = tf.fill(value=-1.0, dims=(3, ))\n",
    "print(ref)\n",
    "print(tf.scatter_update(ref=ref, indices=tf.convert_to_tensor([0, 1, 2]), updates=upd))\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=102, shape=(3,), dtype=float32, numpy=array([-1., -1., -1.], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=117, shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LDA parameters\n",
    "def initialize_variables(K, V, D, alpha=1e-1, eta=1e-1, seed=2014):\n",
    "    \"\"\"\n",
    "    Initialize parameters of LDA model returning adequate Tensors.\n",
    "\n",
    "    args:\n",
    "    \n",
    "        K (int): number of LDA components \n",
    "        V (int): vocabulary size\n",
    "        D (int): number of documents\n",
    "        alpha (float): hyperparameter for theta prior\n",
    "        eta (float): hyperparameter for beta prior\n",
    "       \n",
    "       \n",
    "    returns:\n",
    "    \n",
    "        eta: [V] tensor with prior parameters (alpha) for beta\n",
    "        lambda: [V, K] tensor with posterior word distribution per class\n",
    "        phi: [D, V, K] tensor with vocabulary membership per document\n",
    "        gamma: [D, K] tensor\n",
    "        \n",
    "    \"\"\"\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    eta = tfe.Variable(tf.ones(V) * eta, name=\"eta\")\n",
    "    alpha = tfe.Variable(tf.ones(K) * alpha, name=\"alpha\")\n",
    "    lambda_ = tfe.Variable(tf.abs(tf.random_normal(shape=(V, K))), name='lambda')\n",
    "    \n",
    "    phi = tfe.Variable(tf.random_normal(shape=(D, V, K)), name=\"phi\")\n",
    "    phi = tf.nn.softmax(phi, axis=2)\n",
    "    \n",
    "    gamma = tfe.Variable(tf.abs(tf.random_normal(shape=(D, K))))\n",
    "    \n",
    "    return eta, alpha, lambda_, phi, gamma\n",
    "\n",
    "# test\n",
    "eta, alpha, lambda_, phi, gamma = initialize_variables(K, V, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'lambda:0' shape=(10, 3) dtype=float32, numpy=\n",
       "array([[0.30049554, 1.3264463 , 0.3345    ],\n",
       "       [0.41716772, 0.47891214, 0.60391957],\n",
       "       [0.52135056, 1.1167537 , 2.0634964 ],\n",
       "       [0.3132391 , 0.06342854, 0.98612607],\n",
       "       [0.07255968, 1.0613515 , 0.6355119 ],\n",
       "       [0.07127264, 1.4961201 , 0.63234806],\n",
       "       [0.8790525 , 0.26309258, 0.26927882],\n",
       "       [0.4351647 , 2.0731583 , 0.9537371 ],\n",
       "       [1.8273319 , 0.38898602, 0.48102027],\n",
       "       [0.6974121 , 2.812919  , 0.0922653 ]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda update\n",
    "def update_lambda(lambda_, eta, phi, nw):\n",
    "    \n",
    "    K = len(lambda_)\n",
    "    for k in range(K):\n",
    "        lambda_[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=0, keepdims=True), eta)\n",
    "        \n",
    "    return lambda_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,0], nw * 0.0), axis=0, keepdims=False), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=224, shape=(10, 2), dtype=int32, numpy=\n",
       "array([[0, 0],\n",
       "       [1, 0],\n",
       "       [2, 0],\n",
       "       [3, 0],\n",
       "       [4, 0],\n",
       "       [5, 0],\n",
       "       [6, 0],\n",
       "       [7, 0],\n",
       "       [8, 0],\n",
       "       [9, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tf.meshgrid(list(range(10)), [0], indexing=\"ij\")\n",
    "res = tf.squeeze(tf.stack(res, axis=1), axis=2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambda_)\n",
    "lambda_ = tf.scatter_update(\n",
    "        ref=lambda_, \n",
    "        indices=res, \n",
    "        updates=tf.add(tf.reduce_sum(tf.multiply(phi[:,:,0], nw * 0.0), axis=0, keepdims=False), 0.0))\n",
    "print(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.scatter_update(ref=lambda_, indices=res, updates=upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.scatter_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation using TensorFlow (Batch VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "v1.6.0-0-gd2e24b6039\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.__git_version__)\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Latent Dirichlet Allocation model:\n",
    "\n",
    " - $\\theta_{d=1,...,M} \\sim \\mathrm{Dir}_K(\\alpha)$\n",
    " - $\\beta_{k=1,...,K} \\sim \\mathrm{Dir}_V(\\eta)$\n",
    " - $z_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ K}(\\theta_d)$\n",
    " - $w_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ V}(\\beta_{z_{di}})$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    " - $V$ is a vocabulary size\n",
    " - $K$ is a number of topics\n",
    " - $\\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution (known and symetric)\n",
    " - $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions (known and symetric)\n",
    " - $\\theta_d$ is the topic distribution for document d\n",
    " - $\\beta_k$ is the word distribution for topic k\n",
    " - $z_{di}$ is the topic for the i-th word in document d\n",
    " - $w_{di}$ is a specific word from d-th document belonging to $V$\n",
    "\n",
    "In plate notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hustlin_erd](LDA_plate_notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume full factorial design:\n",
    "\n",
    "\n",
    "$$q(z, \\beta, \\theta) = \\prod_d \\prod_i q(z_{di}) \\times \\prod_d q(\\theta_d) \\times \\prod_k q(\\beta_k)$$\n",
    "\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $q(z_{di}) = p(z_{di}|\\phi) = \\mathrm{Multinomial}_{ \\ K}(\\phi_{dw_{di}}) $\n",
    "- $q(\\theta_d) = p(\\theta_d|\\gamma) = \\mathrm{Dir}_{K}(\\gamma_{d}) $\n",
    "- $q(\\beta_k) = p(\\beta_k|\\lambda) = \\mathrm{Dir}_{V}(\\lambda_{k}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vocabulary to topic membership:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi = \\phi_{d=1,...,D, \\ v=1,...,V, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times V \\times K}$$\n",
    "$ \\forall_{d \\in 1,...,D} \\ \\phi_d $ is a *stochastic matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distribution of topics:*\n",
    "$$ \\gamma = \\gamma_{d=1,...,D, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Within topic vocabulary profile:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\lambda = \\lambda_{k=1,...,K \\ v=1,...,V} \\in \\mathbb{R}_{+}^{K \\times V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of the **E**vidence **L**ower **Bo**und:\n",
    "\n",
    "$$ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) := \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log p(w, z, \\theta, \\beta| \\alpha, \\eta) \\right] - \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log q(z, \\theta, \\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probability factorization of the LDA model\n",
    "\n",
    "$$ \\log p(w, z, \\theta, \\beta| \\alpha, \\eta) = \\log p(w|z, \\beta) p(z|\\theta) p(\\theta | \\alpha) p(\\beta | \\eta) = $$\n",
    "\n",
    "$$ = \\log \\left( \\prod_{k=1}^K p(\\beta_k|\\eta) \\times \\prod_{d=1}^D p(\\theta_d|\\alpha) \\times \\prod_{d=1}^{D} \\prod_{i=1}^{N_d} p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\log p(\\theta_d|\\alpha) + \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log \\left( p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) + \\frac{1}{D} \\sum_{k=1}^K \\log p(\\beta_k|\\eta) \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the expectations with respect to the posterior distribution results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log p(\\theta_d|\\alpha) \\right] = C_1(\\alpha) + \\sum_{k=1}^K (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\beta} \\left[ \\log p(w_{di}|\\beta_{z_{di}}) \\right] =  \\mathbb{E}_{z, \\beta} \\left[ \\log \\beta_{z_{di} i} \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\theta} \\left[ \\log p(z_{di}|\\theta_d) \\right] =  \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log p(\\beta_{k}|\\eta) \\right] = C_2(\\eta) + \\sum_{v=1}^{V} (\\eta_v - 1) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entropy of the posterior distribution we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log\\left(\\prod_{d=1}^{D} \\prod_{i=1}^{N_{d}} q(z_{di}) \\times \\prod_{d=1}^D q(\\theta_d) \\times \\prod_{k=1}^{K} q(\\beta_k)\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\left( \\sum_{i=1}^{N_{d}} \\log q(z_{di}) + \\log q(\\theta_d) + \\frac{1}{D} \\sum_{k=1}^{K} \\log q(\\beta_k)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and expectations with respect to the posterior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z} \\left[ \\log q(z_{di})  \\right] = \\mathbb{E}_{z_{di}} \\left[ \\mathbb{1}_{[z_{di}=k]} \\log \\phi_{dw_{di}k}  \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\log \\phi_{dw_{di}k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log q(\\theta_d)  \\right] = \\mathbb{E}_{\\theta} \\left[ \\log \\frac{1}{B(\\gamma_d)} \\prod_{k=1}^{K} \\theta_{dk}^{\\gamma_{dk} - 1} \\right] = -\\log B(\\gamma_d) + \\sum_{k=1}^K (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta}\\left[ \\log q(\\beta_k) \\right] = -\\log B(\\lambda_k) + \\sum_{v=1}^V (\\lambda_{kv} - 1)\\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We should then be able to collapse ELBO to the formula based on counts of words rather than words from documents. This will show that the LDA model is fixed size method.* \n",
    "\n",
    "*\"When training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\"* (Data Camp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping terms by corresponding parameters we get:\n",
    "\n",
    "#### Document vocabulary membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\phi_{dv}]} = \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\sum_{k=1}^K \\phi_{dw_{di}k} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dw_{di}k}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n_{dv}$ is a number of times word $v$ from vocabulary $V$ was counted in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\gamma_{d}]} = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\log B(\\gamma_d) - (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{D} \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K \\left\\{ (\\eta_v - \\lambda_{kv}) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k} \\right] + \\frac{1}{V} \\log B(\\lambda_k) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate ascent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for every document and vocabulary entry separately we get:\n",
    "\n",
    "#### Document vocabulary membership:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\mathcal{L}_{[\\phi_{dv}]} =  \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ =\\mathrm{argmax}_{\\phi_{dv} \\in \\mathbb{R}_{+}^K} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) + \\xi (\\sum_{k=1}^K \\phi_{dvk} - 1) = \\Psi(\\phi_{dv}, \\xi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\xi} \\Psi(\\phi_{dv}, \\xi) = \\sum_{k=1}^K \\phi_{dvk} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = 0 \\iff n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 + \\xi' = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\log \\phi_{dvk} = \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - 1 + \\xi'  \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doucuments' distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$ \\log B(\\gamma_d) = \\log \\frac{\\prod_{k=1}^{K}\\Gamma(\\gamma_{dk})}{\\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})} = \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\mathcal{L}_{[\\gamma_{d}]} = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} + \\log B(\\gamma_d) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) (\\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}}) = \\Psi(\\gamma_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = -\\psi(\\gamma_{dj}) + \\psi({\\sum_k \\gamma_{dk})} + \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\psi(\\gamma_{dj}) - \\psi(\\sum_k \\gamma_{dk}) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *trigamma* function has no real roots it follows that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = 0 \\iff \\forall_{k} \\ \\ \\  \\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk} = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff   \\gamma_{dk} = \\alpha_k  + \\sum_{v=1}^V n_{dv} \\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile (M-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact derivation as for the $\\gamma_d$ gives an update equation for $\\lambda_k$\n",
    "\n",
    "$$ \\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - initialize $\\lambda$ and $\\gamma$\n",
    " - while improvement $ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) > 1e-6$ \n",
    "     - for $d = 1,...,D$\n",
    "         - $\\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\}$\n",
    "         - $\\gamma_{dk} = \\alpha  + \\sum_{v=1}^V n_{dv} \\phi_{dvk}$\n",
    "     - $\\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "round_() got an unexpected keyword argument 'digits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-a45132976314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: round_() got an unexpected keyword argument 'digits'"
     ]
    }
   ],
   "source": [
    "# set data dimensions\n",
    "K = 3\n",
    "V = 5\n",
    "D = 1000\n",
    "N = 1000\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(2014)\n",
    "\n",
    "# beta prior parameters\n",
    "eta = np.ones(V) * 1e-2\n",
    "\n",
    "# beta profiles\n",
    "beta = np.random.dirichlet(alpha=eta, size=K)\n",
    "\n",
    "# theta prior parameters\n",
    "alpha = np.ones(K) * 1e-2\n",
    "# alpha[0] = 10\n",
    "\n",
    "# document's prior topic allocation\n",
    "theta = np.random.dirichlet(alpha=alpha, size=D)\n",
    "\n",
    "# word's topic membership\n",
    "z = [np.random.choice(K, size=N, replace=True, p=theta[d, :]) for d in range(D)]\n",
    "z = np.vstack(z)\n",
    "\n",
    "# actual words and counts\n",
    "w = [np.array([np.random.choice(V, size=1, p=beta[k,:])[0] for k in z[d, :]]  + list(range(V))) for d in range(D)]\n",
    "nw = [np.unique(w[d], return_counts=True)[1] for d in range(D)]\n",
    "nw = np.vstack(nw)\n",
    "w = np.vstack(w)\n",
    "\n",
    "nw = tf.convert_to_tensor(nw, dtype=tf.float32)\n",
    "# nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(beta, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model\n",
    "\n",
    "Few notes:\n",
    "- there is a tape now for gradients like in pyTorch\n",
    "- we will only use tensorflow for fast tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7042170, shape=(1, 5), dtype=float32, numpy=array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize LDA parameters\n",
    "def initialize_parameters(K, V, D, alpha=1e-2, eta=1e-2, seed=2014):\n",
    "    \"\"\"\n",
    "    Initialize parameters of LDA model returning adequate Tensors.\n",
    "\n",
    "    args:\n",
    "    \n",
    "        K (int): number of LDA components \n",
    "        V (int): vocabulary size\n",
    "        D (int): number of documents\n",
    "        alpha (float): hyperparameter for theta prior\n",
    "        eta (float): hyperparameter for beta prior\n",
    "       \n",
    "       \n",
    "    returns:\n",
    "    \n",
    "        eta: [1 x V] tensor with prior parameters (alpha) for beta\n",
    "        lambda: k x [1, V] tensors with posterior word distribution per class\n",
    "        phi: [D, V, K] tensor with vocabulary membership per document\n",
    "        gamma: k x [D, 1] tensors\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    eta = tf.ones((1, V)) * eta\n",
    "    alpha = tf.ones((K, 1)) * alpha\n",
    "    \n",
    "    lambda_ = [tf.abs(tf.random_normal(shape=(1, V), seed=seed)) for k in range(K)]\n",
    "    \n",
    "    phi = tf.random_normal(shape=(D, V, K), seed=seed)\n",
    "    phi = tf.nn.softmax(phi, axis=2)\n",
    "    \n",
    "    gamma = [tf.abs(tf.random_normal(shape=(D, 1), seed=seed)) for k in range(K)]\n",
    "    \n",
    "    return eta, alpha, lambda_, phi, gamma\n",
    "\n",
    "# test\n",
    "eta, alpha, lambda_, phi, gamma = initialize_parameters(K, V, D)\n",
    "eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda update\n",
    "def update_lambda(lambda_, eta, phi, nw):\n",
    "    \n",
    "    K = len(lambda_)\n",
    "    for k in range(K):\n",
    "        lambda_[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=0, keepdims=True), eta)\n",
    "        \n",
    "    return lambda_ \n",
    "\n",
    "# test\n",
    "# print(lambda_)\n",
    "# update_lambda(lambda_, eta, phi, nw)\n",
    "# print(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma update\n",
    "def update_gamma(gamma, alpha, phi, nw):\n",
    "    \n",
    "    K = len(gamma)\n",
    "    for k in range(K):\n",
    "        gamma[k] = tf.add(tf.reduce_sum(tf.multiply(phi[:,:,k], nw), axis=1, keepdims=True), alpha[k])\n",
    "        \n",
    "    return gamma\n",
    "\n",
    "# test\n",
    "# print(gamma)\n",
    "# update_gamma(gamma, alpha, phi, nw)\n",
    "# print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right]  = \\psi(\\lambda_{kv}) - \\psi(\\sum_{v=1}^V \\lambda_{kv}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_log_beta(lambda_):\n",
    "    return [tf.digamma(lam) - tf.digamma(tf.reduce_sum(lam)) for lam in lambda_]\n",
    "\n",
    "# e_log_beta(lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_log_theta(gamma):\n",
    "    return [tf.digamma(g_k) - tf.digamma(tf.reduce_sum(g_k)) for g_k in gamma]\n",
    "\n",
    "# e_log_theta(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_phi(lambda_, gamma, D, V):\n",
    "    eb_1xvxk = tf.stack(e_log_beta(lambda_), axis=2)\n",
    "    eb_dxvxk = tf.tile(eb_1xvxk, multiples=[D, 1, 1])\n",
    "    et_dx1xk = tf.stack(e_log_theta(gamma), axis=2)\n",
    "    et_dxvxk = tf.tile(et_dx1xk, multiples=[1, V, 1])\n",
    "    phi_dxvxk = tf.nn.softmax(logits=eb_dxvxk + et_dxvxk, axis=2)\n",
    "    return phi_dxvxk\n",
    "\n",
    "# update_phi(lambda_, gamma, D, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it all up together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta, alpha, lambda_, phi, gamma = initialize_parameters(K, V, D, seed=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "tf.Tensor([0.10093229 0.3090393  0.5900351 ], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10092768 0.3089348  0.59013754], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10092317 0.30883005 0.59026426], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10092156 0.3087177  0.59036404], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10091706 0.30861768 0.5904741 ], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.1009163 0.308513  0.5905758], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10091124 0.30840108 0.5906898 ], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10090953 0.30830276 0.5907991 ], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10090821 0.30820018 0.5909    ], shape=(3,), dtype=float32)\n",
      "----------------------------------------\n",
      "tf.Tensor([0.10090455 0.30809584 0.5909973 ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(100):\n",
    "        # E-Step:\n",
    "        gamma = [tf.fill(g_k.shape, 1.0) for g_k in gamma]\n",
    "        phi = update_phi(lambda_, gamma, D, V)\n",
    "        gamma = update_gamma(gamma, alpha, phi, nw)\n",
    "        \n",
    "    lambda_ = update_lambda(lambda_, eta, phi, nw)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(tf.reduce_mean(phi, axis=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(beta, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.979, 0.   , 0.   , 0.018, 0.003],\n",
       "       [0.004, 0.003, 0.003, 0.99 , 0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.vstack([(lam / tf.reduce_sum(lam)).numpy()  for lam in lambda_]), decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

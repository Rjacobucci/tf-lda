{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation using TensorFlow (Batch VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Latent Dirichlet Allocation model:\n",
    "\n",
    " - $\\theta_{d=1,...,M} \\sim \\mathrm{Dir}_K(\\alpha)$\n",
    " - $\\beta_{k=1,...,K} \\sim \\mathrm{Dir}_V(\\eta)$\n",
    " - $z_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ K}(\\theta_d)$\n",
    " - $w_{d=1,...,M,i=1,...,N_d} \\sim \\mathrm{Multinomial}_{ \\ V}(\\beta_{z_{di}})$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    " - $V$ is a vocabulary size\n",
    " - $K$ is a number of topics\n",
    " - $\\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution (known and symetric)\n",
    " - $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions (known and symetric)\n",
    " - $\\theta_d$ is the topic distribution for document d\n",
    " - $\\beta_k$ is the word distribution for topic k\n",
    " - $z_{di}$ is the topic for the i-th word in document d\n",
    " - $w_{di}$ is a specific word from d-th document belonging to $V$\n",
    "\n",
    "In plate notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hustlin_erd](LDA_plate_notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume full factorial design:\n",
    "\n",
    "\n",
    "$$q(z, \\beta, \\theta) = \\prod_d \\prod_i q(z_{di}) \\times \\prod_d q(\\theta_d) \\times \\prod_k q(\\beta_k)$$\n",
    "\n",
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $q(z_{di}) = p(z_{di}|\\phi) = \\mathrm{Multinomial}_{ \\ K}(\\phi_{dw_{di}}) $\n",
    "- $q(\\theta_d) = p(\\theta_d|\\gamma) = \\mathrm{Dir}_{K}(\\gamma_{d}) $\n",
    "- $q(\\beta_k) = p(\\beta_k|\\lambda) = \\mathrm{Dir}_{V}(\\lambda_{k}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Vocabulary to topic membership:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\phi = \\phi_{d=1,...,D, \\ v=1,...,V, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times V \\times K}$$\n",
    "$ \\forall_{d \\in 1,...,D} \\ \\phi_d $ is a *stochastic matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distribution of topics:*\n",
    "$$ \\gamma = \\gamma_{d=1,...,D, \\ k=1,...,K} \\in \\mathbb{R}_{+}^{D \\times K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Within topic vocabulary profile:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\lambda = \\lambda_{k=1,...,K \\ v=1,...,V} \\in \\mathbb{R}_{+}^{K \\times V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of the **E**vidence **L**ower **Bo**und:\n",
    "\n",
    "$$ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) := \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log p(w, z, \\theta, \\beta| \\alpha, \\eta) \\right] - \\mathbb{E}_{z, \\theta, \\beta}\\left[\\log q(z, \\theta, \\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probability factorization of the LDA model\n",
    "\n",
    "$$ \\log p(w, z, \\theta, \\beta| \\alpha, \\eta) = \\log p(w|z, \\beta) p(z|\\theta) p(\\theta | \\alpha) p(\\beta | \\eta) = $$\n",
    "\n",
    "$$ = \\log \\left( \\prod_{k=1}^K p(\\beta_k|\\eta) \\times \\prod_{d=1}^D p(\\theta_d|\\alpha) \\times \\prod_{d=1}^{D} \\prod_{i=1}^{N_d} p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\log p(\\theta_d|\\alpha) + \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) = $$\n",
    "\n",
    "$$ =  \\sum_{k=1}^K \\log p(\\beta_k|\\eta) +  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) = $$\n",
    "\n",
    "$$ =  \\sum_{d=1}^D \\left( \\log p(\\theta_d|\\alpha) + \\sum_{i=1}^{N_d} \\log \\left( p(w_{di}|\\beta_{z_{di}}) p(z_{di}|\\theta_d) \\right) + \\frac{1}{D} \\sum_{k=1}^K \\log p(\\beta_k|\\eta) \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the expectations with respect to the posterior distribution results in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log p(\\theta_d|\\alpha) \\right] = C_1(\\alpha) + \\sum_{k=1}^K (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\beta} \\left[ \\log p(w_{di}|\\beta_{z_{di}}) \\right] =  \\mathbb{E}_{z, \\beta} \\left[ \\log \\beta_{z_{di} i} \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z, \\theta} \\left[ \\log p(z_{di}|\\theta_d) \\right] =  \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta} \\left[ \\log p(\\beta_{k}|\\eta) \\right] = C_2(\\eta) + \\sum_{v=1}^{V} (\\eta_v - 1) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entropy of the posterior distribution we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\log\\left(\\prod_{d=1}^{D} \\prod_{i=1}^{N_{d}} q(z_{di}) \\times \\prod_{d=1}^D q(\\theta_d) \\times \\prod_{k=1}^{K} q(\\beta_k)\\right) =  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\left( \\sum_{i=1}^{N_{d}} \\log q(z_{di}) + \\log q(\\theta_d) + \\frac{1}{D} \\sum_{k=1}^{K} \\log q(\\beta_k)\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and expectations with respect to the posterior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{z} \\left[ \\log q(z_{di})  \\right] = \\mathbb{E}_{z_{di}} \\left[ \\mathbb{1}_{[z_{di}=k]} \\log \\phi_{dw_{di}k}  \\right] = \\sum_{k=1}^{K} \\phi_{dw_{di}k} \\log \\phi_{dw_{di}k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log q(\\theta_d)  \\right] = \\mathbb{E}_{\\theta} \\left[ \\log \\frac{1}{B(\\gamma_d)} \\prod_{k=1}^{K} \\theta_{dk}^{\\gamma_{dk} - 1} \\right] = -\\log B(\\gamma_d) + \\sum_{k=1}^K (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\beta}\\left[ \\log q(\\beta_k) \\right] = -\\log B(\\lambda_k) + \\sum_{v=1}^V (\\lambda_{kv} - 1)\\mathbb{E}_{\\beta} \\left[ \\log \\beta_{kv} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We should then be able to collapse ELBO to the formula based on counts of words rather than words from documents. This will show that the LDA model is fixed size method.* \n",
    "\n",
    "*\"When training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\"* (Data Camp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping terms by corresponding parameters we get:\n",
    "\n",
    "#### Document vocabulary membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\phi_{dv}]} = \\sum_{d=1}^{D} \\sum_{i=1}^{N_d} \\sum_{k=1}^K \\phi_{dw_{di}k} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k i} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dw_{di}k}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n_{dv}$ is a number of times word $v$ from vocabulary $V$ was counted in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}_{[\\gamma_{d}]} = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - 1) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\log B(\\gamma_d) - (\\gamma_{dk} - 1)\\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{d=1}^{D} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] + \\frac{1}{K} \\log B(\\gamma_d) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{D} \\sum_{d=1}^{D} \\sum_{v=1}^{V} \\sum_{k=1}^K \\left\\{ (\\eta_v - \\lambda_{kv}) \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k} \\right] + \\frac{1}{V} \\log B(\\lambda_k) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate ascent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing for every document and vocabulary entry separately we get:\n",
    "\n",
    "#### Document vocabulary membership:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\mathcal{L}_{[\\phi_{dv}]} =  \\mathrm{argmax}_{\\phi_{dv} \\in \\Delta^{K-1}} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ =\\mathrm{argmax}_{\\phi_{dv} \\in \\mathbb{R}_{+}^K} \\sum_{k=1}^K n_{dv} \\phi_{dvk} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk}  \\right) + \\xi (\\sum_{k=1}^K \\phi_{dvk} - 1) = \\Psi(\\phi_{dv}, \\xi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\xi} \\Psi(\\phi_{dv}, \\xi) = \\sum_{k=1}^K \\phi_{dvk} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\phi_{dvk}} \\Psi(\\phi_{dv}, \\xi) = 0 \\iff n_{dv} \\left( \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 \\right) + \\xi = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - \\log \\phi_{dvk} - 1 + \\xi' = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\log \\phi_{dvk} = \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] - 1 + \\xi'  \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff \\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doucuments' distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] = \\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$ \\log B(\\gamma_d) = \\log \\frac{\\prod_{k=1}^{K}\\Gamma(\\gamma_{dk})}{\\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})} = \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\mathcal{L}_{[\\gamma_{d}]} = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{dk} \\right] \\right\\} + \\log B(\\gamma_d) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\mathrm{argmax}_{\\gamma_{d} \\in \\mathbb{R}_{+}^{K}} \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk}) (\\psi(\\gamma_{dk}) - \\psi(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\sum_{k=1}^{K}\\log \\Gamma (\\gamma_{dk}) - \\log \\Gamma ({\\sum_{k=1}^K \\gamma_{dk}}) = \\Psi(\\gamma_d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = -\\psi(\\gamma_{dj}) + \\psi({\\sum_k \\gamma_{dk})} + \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} + \\psi(\\gamma_{dj}) - \\psi(\\sum_k \\gamma_{dk}) = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{k=1}^K \\left\\{ (\\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk})*(\\psi'(\\gamma_{dj})*1_{k=j} - \\psi'(\\sum_{k=1}^K\\gamma_{dk})) \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *trigamma* function has no real roots it follows that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\gamma_{dj}} \\Psi(\\gamma_d) = 0 \\iff \\forall_{k} \\ \\ \\  \\alpha_k - \\gamma_{dk} + \\sum_{v=1}^V n_{dv} \\phi_{dvk} = 0 \\iff $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\iff   \\gamma_{dk} = \\alpha_k  + \\sum_{v=1}^V n_{dv} \\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vocabulary profile (M-Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact derivation as for the $\\gamma_d$ gives an update equation for $\\lambda_k$\n",
    "\n",
    "$$ \\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - initialize $\\lambda$ and $\\gamma$\n",
    " - while improvement $ \\mathcal{L}(w, \\phi, \\gamma, \\lambda) > 1e-6$ \n",
    "     - for $d = 1,...,D$\n",
    "         - $\\phi_{dvk} \\propto \\exp \\left\\{ \\mathbb{E}_{\\beta} \\left[ \\log \\beta_{k v} \\right] + \\mathbb{E}_{\\theta} \\left[ \\log \\theta_{d k} \\right] \\right\\}$\n",
    "         - $\\gamma_{dk} = \\alpha  + \\sum_{v=1}^V n_{dv} \\phi_{dvk}$\n",
    "     - $\\lambda_{kv} = \\eta_v + \\sum_{d=1}^D n_{dv}\\phi_{dvk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data dimensions\n",
    "K = 2\n",
    "V = 5\n",
    "D = 5\n",
    "N = 100\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(2014)\n",
    "\n",
    "# beta prior parameters\n",
    "eta = np.ones(V)\n",
    "\n",
    "# beta profiles\n",
    "beta = np.random.dirichlet(alpha=eta, size=K)\n",
    "\n",
    "# theta prior parameters\n",
    "alpha = np.ones(K)\n",
    "\n",
    "# document's prior topic allocation\n",
    "theta = np.random.dirichlet(alpha=alpha, size=D)\n",
    "\n",
    "# word's topic membership\n",
    "z = [np.random.choice(K, size=N, replace=True, p=theta[d, :]) for d in range(D)]\n",
    "z = np.vstack(z)\n",
    "\n",
    "# actual words and counts\n",
    "w = [np.array([np.random.choice(V, size=1, p=beta[k,:])[0] for k in z[d, :]]) for d in range(D)]\n",
    "nw = [np.unique(w[d], return_counts=True)[1] for d in range(D)]\n",
    "nw = np.vstack(nw)\n",
    "w = np.vstack(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model\n",
    "\n",
    "Few notes:\n",
    "- there is a tape now for gradients like in pyTorch\n",
    "- we will only use tensorflow for fast tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=13, shape=(2, 5), dtype=float64, numpy=\n",
       "array([[0.06741897, 0.11791967, 0.13534136, 1.51368743, 0.16563257],\n",
       "       [0.39691799, 0.37045781, 0.06175507, 0.15508404, 1.01578509]])>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_beta = tf.convert_to_tensor(beta, name=\"beta\")\n",
    "tf.add(tf_beta, tf_beta)\n",
    "\n",
    "# def estep_update_theta():\n",
    "    \n",
    "#     return tf_beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=17, shape=(), dtype=float32, numpy=0.4227842>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.digamma(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
